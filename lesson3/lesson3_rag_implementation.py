#!/usr/bin/env python3
"""
AI ì±—ë´‡ ë©˜í† ë§ - 3ì°¨ì‹œ: RAG ì‹œìŠ¤í…œ êµ¬í˜„
Author: AI Chatbot Workshop
Date: 2024-08-30
Description: ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) ì±—ë´‡ êµ¬í˜„
            PDF/TXT ë¬¸ì„œ ì²˜ë¦¬, ë²¡í„° ì„ë² ë”©, ìœ ì‚¬ë„ ê²€ìƒ‰, ë‹µë³€ ìƒì„±
"""

import os
import sys
import logging
import json
import time
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import asyncio
from datetime import datetime
import hashlib
import uuid

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ imports
import streamlit as st
import typer
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
import openai
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# ë¬¸ì„œ ì²˜ë¦¬ imports
import pypdf
from docx import Document
import markdown

# ë²¡í„° DB imports
import chromadb
from chromadb.config import Settings
import faiss

# ì„¤ì • íŒŒì¼ import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import get_config

# ì „ì—­ ì„¤ì • ë° ë¡œê¹…
config = get_config()
console = Console()
app = typer.Typer(help="RAG ì±—ë´‡ êµ¬í˜„ - ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/lesson3_rag.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class DocumentChunk:
    """ë¬¸ì„œ ì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'id': self.id,
            'content': self.content,
            'metadata': self.metadata,
            'embedding': self.embedding
        }

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_id: str
    content: str
    score: float
    metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'chunk_id': self.chunk_id,
            'content': self.content,
            'score': self.score,
            'metadata': self.metadata
        }

@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    query: str
    answer: str
    sources: List[SearchResult]
    processing_time: float
    tokens_used: int
    metadata: Dict[str, Any]

class DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            chunk_size: ì²­í¬ í¬ê¸° (í† í° ê¸°ì¤€)
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„ í¬ê¸°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.supported_extensions = ['.pdf', '.txt', '.docx', '.md']
        
        logger.info(f"ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ - chunk_size: {chunk_size}, overlap: {chunk_overlap}")
    
    def process_file(self, file_path: str) -> List[DocumentChunk]:
        """
        íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• 
        
        Args:
            file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            List[DocumentChunk]: ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ë“¤
            
        Raises:
            ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹
            FileNotFoundError: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
        """
        start_time = time.time()
        logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path}")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        file_ext = Path(file_path).suffix.lower()
        if file_ext not in self.supported_extensions:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")
        
        # íŒŒì¼ ì½ê¸°
        try:
            if file_ext == '.pdf':
                text_content = self._extract_pdf(file_path)
            elif file_ext == '.txt':
                text_content = self._extract_txt(file_path)
            elif file_ext == '.docx':
                text_content = self._extract_docx(file_path)
            elif file_ext == '.md':
                text_content = self._extract_markdown(file_path)
            else:
                raise ValueError(f"ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")
            
            logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ - ê¸¸ì´: {len(text_content)}ì")
            
            # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
            chunks = self._smart_chunking(text_content, file_path)
            
            processing_time = time.time() - start_time
            logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ - ì²­í¬ ìˆ˜: {len(chunks)}, ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            
            return chunks
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def _extract_pdf(self, file_path: str) -> str:
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        logger.debug(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {file_path}")
        
        text_content = []
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = pypdf.PdfReader(file)
                
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    if page_text.strip():  # ë¹ˆ í˜ì´ì§€ ì œì™¸
                        text_content.append(f"[Page {page_num + 1}]\n{page_text}")
                        logger.debug(f"í˜ì´ì§€ {page_num + 1} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
                
                full_text = '\n\n'.join(text_content)
                logger.info(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ - í˜ì´ì§€ ìˆ˜: {len(pdf_reader.pages)}")
                
                return full_text
                
        except Exception as e:
            logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _extract_txt(self, file_path: str) -> str:
        """TXT íŒŒì¼ ì½ê¸°"""
        logger.debug(f"TXT íŒŒì¼ ì½ê¸°: {file_path}")
        
        try:
            # ì¸ì½”ë”© ìë™ ê°ì§€ ì‹œë„
            encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-16']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as file:
                        content = file.read()
                    logger.info(f"TXT íŒŒì¼ ì½ê¸° ì™„ë£Œ - ì¸ì½”ë”©: {encoding}")
                    return content
                except UnicodeDecodeError:
                    continue
            
            raise ValueError("ì§€ì›í•˜ëŠ” ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        except Exception as e:
            logger.error(f"TXT íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _extract_docx(self, file_path: str) -> str:
        """DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        logger.debug(f"DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {file_path}")
        
        try:
            doc = Document(file_path)
            paragraphs = [paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()]
            
            full_text = '\n'.join(paragraphs)
            logger.info(f"DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ - ë¬¸ë‹¨ ìˆ˜: {len(paragraphs)}")
            
            return full_text
            
        except Exception as e:
            logger.error(f"DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _extract_markdown(self, file_path: str) -> str:
        """Markdown íŒŒì¼ ì²˜ë¦¬"""
        logger.debug(f"Markdown íŒŒì¼ ì²˜ë¦¬: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                md_content = file.read()
            
            # HTMLë¡œ ë³€í™˜ í›„ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ì„ íƒì‚¬í•­)
            # ì—¬ê¸°ì„œëŠ” ì›ë³¸ ë§ˆí¬ë‹¤ìš´ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            logger.info("Markdown íŒŒì¼ ì½ê¸° ì™„ë£Œ")
            return md_content
            
        except Exception as e:
            logger.error(f"Markdown íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _smart_chunking(self, text: str, file_path: str) -> List[DocumentChunk]:
        """
        ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì•Œê³ ë¦¬ì¦˜
        ë¬¸ë‹¨, ë¬¸ì¥ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ì§€ëŠ¥ì  ë¶„í• 
        
        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            file_path: ì›ë³¸ íŒŒì¼ ê²½ë¡œ (ë©”íƒ€ë°ì´í„°ìš©)
            
        Returns:
            List[DocumentChunk]: ë¶„í• ëœ ì²­í¬ë“¤
        """
        logger.debug("ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì‹œì‘")
        
        # 1ë‹¨ê³„: ë¬¸ë‹¨ ê¸°ì¤€ ë¶„í•  ì‹œë„
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks = []
        current_chunk = ""
        chunk_index = 0
        
        for paragraph in paragraphs:
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í–ˆì„ ë•Œ í¬ê¸° í™•ì¸
            potential_chunk = current_chunk + '\n\n' + paragraph if current_chunk else paragraph
            
            if len(potential_chunk) <= self.chunk_size:
                # í¬ê¸°ê°€ í—ˆìš© ë²”ìœ„ ë‚´ë©´ ì¶”ê°€
                current_chunk = potential_chunk
            else:
                # í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
                if current_chunk:
                    chunk = self._create_chunk(
                        current_chunk, 
                        chunk_index, 
                        file_path
                    )
                    chunks.append(chunk)
                    chunk_index += 1
                
                # ë‹¨ì¼ ë¬¸ë‹¨ì´ ë„ˆë¬´ í´ ê²½ìš° ë¬¸ì¥ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
                if len(paragraph) > self.chunk_size:
                    sub_chunks = self._split_by_sentences(paragraph, chunk_index, file_path)
                    chunks.extend(sub_chunks)
                    chunk_index += len(sub_chunks)
                    current_chunk = ""
                else:
                    current_chunk = paragraph
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunk = self._create_chunk(current_chunk, chunk_index, file_path)
            chunks.append(chunk)
        
        logger.info(f"ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì™„ë£Œ - ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
        
        # ì²­í‚¹ ê²°ê³¼ ë¡œê¹…
        for i, chunk in enumerate(chunks):
            logger.debug(f"ì²­í¬ {i}: ê¸¸ì´ {len(chunk.content)}ì, ID: {chunk.id}")
        
        return chunks
    
    def _split_by_sentences(self, text: str, start_index: int, file_path: str) -> List[DocumentChunk]:
        """ë¬¸ì¥ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í• """
        logger.debug("ë¬¸ì¥ ê¸°ì¤€ ë¶„í•  ì‹œì‘")
        
        # ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„í•  (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© ê°€ëŠ¥)
        sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]
        chunks = []
        current_chunk = ""
        chunk_index = start_index
        
        for sentence in sentences:
            potential_chunk = current_chunk + ' ' + sentence if current_chunk else sentence
            
            if len(potential_chunk) <= self.chunk_size:
                current_chunk = potential_chunk
            else:
                if current_chunk:
                    chunk = self._create_chunk(current_chunk, chunk_index, file_path)
                    chunks.append(chunk)
                    chunk_index += 1
                current_chunk = sentence
        
        if current_chunk:
            chunk = self._create_chunk(current_chunk, chunk_index, file_path)
            chunks.append(chunk)
        
        logger.debug(f"ë¬¸ì¥ ê¸°ì¤€ ë¶„í•  ì™„ë£Œ - ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
        return chunks
    
    def _create_chunk(self, content: str, index: int, file_path: str) -> DocumentChunk:
        """ì²­í¬ ê°ì²´ ìƒì„±"""
        chunk_id = self._generate_chunk_id(content, index)
        
        metadata = {
            'source_file': os.path.basename(file_path),
            'file_path': file_path,
            'chunk_index': index,
            'length': len(content),
            'created_at': datetime.now().isoformat(),
            'word_count': len(content.split()),
            'line_count': content.count('\n') + 1
        }
        
        return DocumentChunk(
            id=chunk_id,
            content=content,
            metadata=metadata
        )
    
    def _generate_chunk_id(self, content: str, index: int) -> str:
        """ì²­í¬ ID ìƒì„± (ë‚´ìš© í•´ì‹œ + ì¸ë±ìŠ¤)"""
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
        return f"chunk_{index:04d}_{content_hash}"

class EmbeddingManager:
    """ì„ë² ë”© ìƒì„± ë° ê´€ë¦¬"""
    
    def __init__(self, model: str = "text-embedding-3-small"):
        """
        ì„ë² ë”© ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            model: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª…
        """
        self.model = model
        self.client = openai.OpenAI(api_key=config.llm.openai_api_key)
        self.embedding_cache = {}  # ì„ë² ë”© ìºì‹±
        
        logger.info(f"ì„ë² ë”© ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸: {model}")
    
    def generate_embeddings(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """
        ì²­í¬ë“¤ì˜ ì„ë² ë”© ìƒì„±
        
        Args:
            chunks: ì„ë² ë”©ì„ ìƒì„±í•  ì²­í¬ë“¤
            
        Returns:
            List[DocumentChunk]: ì„ë² ë”©ì´ ì¶”ê°€ëœ ì²­í¬ë“¤
        """
        start_time = time.time()
        logger.info(f"ì„ë² ë”© ìƒì„± ì‹œì‘ - ì²­í¬ ìˆ˜: {len(chunks)}")
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
        texts = [chunk.content for chunk in chunks]
        
        try:
            # OpenAI ì„ë² ë”© API í˜¸ì¶œ (ë°°ì¹˜ ì²˜ë¦¬)
            logger.debug(f"OpenAI ì„ë² ë”© API í˜¸ì¶œ - ëª¨ë¸: {self.model}")
            
            response = self.client.embeddings.create(
                model=self.model,
                input=texts
            )
            
            # ì‘ë‹µì—ì„œ ì„ë² ë”© ì¶”ì¶œí•˜ì—¬ ì²­í¬ì— í• ë‹¹
            for i, chunk in enumerate(chunks):
                embedding = response.data[i].embedding
                chunk.embedding = embedding
                
                # ìºì‹±
                self.embedding_cache[chunk.id] = embedding
                
                logger.debug(f"ì²­í¬ {chunk.id} ì„ë² ë”© ìƒì„± ì™„ë£Œ - ì°¨ì›: {len(embedding)}")
            
            processing_time = time.time() - start_time
            tokens_used = response.usage.total_tokens
            
            logger.info(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ, í† í° ì‚¬ìš©: {tokens_used}")
            
            return chunks
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def generate_query_embedding(self, query: str) -> List[float]:
        """
        ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            List[float]: ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
        """
        logger.debug(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±: {query[:50]}...")
        
        # ìºì‹œ í™•ì¸
        query_hash = hashlib.md5(query.encode('utf-8')).hexdigest()
        if query_hash in self.embedding_cache:
            logger.debug("ìºì‹œëœ ì¿¼ë¦¬ ì„ë² ë”© ì‚¬ìš©")
            return self.embedding_cache[query_hash]
        
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=[query]
            )
            
            embedding = response.data[0].embedding
            
            # ìºì‹±
            self.embedding_cache[query_hash] = embedding
            
            logger.debug(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ - ì°¨ì›: {len(embedding)}")
            return embedding
            
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise

class VectorDatabase:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, db_type: str = "chroma", persist_dir: str = "./data/vector_db"):
        """
        ë²¡í„° DB ì´ˆê¸°í™”
        
        Args:
            db_type: ì‚¬ìš©í•  ë²¡í„° DB ìœ í˜• ('chroma' ë˜ëŠ” 'faiss')
            persist_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.db_type = db_type
        self.persist_dir = persist_dir
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(persist_dir, exist_ok=True)
        
        if db_type == "chroma":
            self._init_chroma()
        elif db_type == "faiss":
            self._init_faiss()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë²¡í„° DB ìœ í˜•: {db_type}")
        
        logger.info(f"ë²¡í„° DB ì´ˆê¸°í™” ì™„ë£Œ - ìœ í˜•: {db_type}, ì €ì¥ ê²½ë¡œ: {persist_dir}")
    
    def _init_chroma(self):
        """ChromaDB ì´ˆê¸°í™”"""
        logger.debug("ChromaDB ì´ˆê¸°í™”")
        
        self.client = chromadb.PersistentClient(path=self.persist_dir)
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        self.collection_name = "rag_documents"
        try:
            self.collection = self.client.get_collection(name=self.collection_name)
            logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ: {self.collection_name}")
        except:
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "RAG ë¬¸ì„œ ì»¬ë ‰ì…˜"}
            )
            logger.info(f"ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {self.collection_name}")
    
    def _init_faiss(self):
        """FAISS ì´ˆê¸°í™”"""
        logger.debug("FAISS ì´ˆê¸°í™”")
        
        # FAISS ì¸ë±ìŠ¤ëŠ” ë°ì´í„° ì¶”ê°€ ì‹œì ì— ìƒì„±
        self.index = None
        self.chunk_mapping = {}  # ì¸ë±ìŠ¤ ID -> ì²­í¬ ë§¤í•‘
        self.index_file = os.path.join(self.persist_dir, "faiss.index")
        self.mapping_file = os.path.join(self.persist_dir, "chunk_mapping.json")
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
        if os.path.exists(self.index_file) and os.path.exists(self.mapping_file):
            try:
                self.index = faiss.read_index(self.index_file)
                with open(self.mapping_file, 'r', encoding='utf-8') as f:
                    self.chunk_mapping = json.load(f)
                logger.info("ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±: {str(e)}")
    
    def add_chunks(self, chunks: List[DocumentChunk]):
        """
        ì²­í¬ë“¤ì„ ë²¡í„° DBì— ì¶”ê°€
        
        Args:
            chunks: ì¶”ê°€í•  ì²­í¬ë“¤ (ì„ë² ë”© í¬í•¨)
        """
        logger.info(f"ë²¡í„° DBì— ì²­í¬ ì¶”ê°€ - ì²­í¬ ìˆ˜: {len(chunks)}")
        
        if self.db_type == "chroma":
            self._add_to_chroma(chunks)
        elif self.db_type == "faiss":
            self._add_to_faiss(chunks)
        
        logger.info("ë²¡í„° DB ì¶”ê°€ ì™„ë£Œ")
    
    def _add_to_chroma(self, chunks: List[DocumentChunk]):
        """ChromaDBì— ì²­í¬ ì¶”ê°€"""
        logger.debug("ChromaDBì— ì²­í¬ ì¶”ê°€")
        
        # ë°ì´í„° ì¤€ë¹„
        ids = [chunk.id for chunk in chunks]
        documents = [chunk.content for chunk in chunks]
        embeddings = [chunk.embedding for chunk in chunks]
        metadatas = [chunk.metadata for chunk in chunks]
        
        # ë°°ì¹˜ ì¶”ê°€
        try:
            self.collection.add(
                ids=ids,
                documents=documents,
                embeddings=embeddings,
                metadatas=metadatas
            )
            logger.debug(f"ChromaDB ë°°ì¹˜ ì¶”ê°€ ì™„ë£Œ - {len(chunks)}ê°œ ì²­í¬")
            
        except Exception as e:
            logger.error(f"ChromaDB ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _add_to_faiss(self, chunks: List[DocumentChunk]):
        """FAISSì— ì²­í¬ ì¶”ê°€"""
        logger.debug("FAISSì— ì²­í¬ ì¶”ê°€")
        
        if not chunks[0].embedding:
            raise ValueError("FAISS ì¶”ê°€ë¥¼ ìœ„í•´ì„œëŠ” ì„ë² ë”©ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        embeddings = np.array([chunk.embedding for chunk in chunks]).astype('float32')
        dimension = embeddings.shape[1]
        
        # ì¸ë±ìŠ¤ ìƒì„± (ì²˜ìŒ ì¶”ê°€ ì‹œ)
        if self.index is None:
            self.index = faiss.IndexFlatIP(dimension)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©)
            logger.debug(f"ìƒˆ FAISS ì¸ë±ìŠ¤ ìƒì„± - ì°¨ì›: {dimension}")
        
        # ì²­í¬ ë§¤í•‘ ì—…ë°ì´íŠ¸
        start_idx = len(self.chunk_mapping)
        for i, chunk in enumerate(chunks):
            self.chunk_mapping[str(start_idx + i)] = {
                'id': chunk.id,
                'content': chunk.content,
                'metadata': chunk.metadata
            }
        
        # ì„ë² ë”© ì¶”ê°€
        self.index.add(embeddings)
        
        # ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(self.index, self.index_file)
        with open(self.mapping_file, 'w', encoding='utf-8') as f:
            json.dump(self.chunk_mapping, f, ensure_ascii=False, indent=2)
        
        logger.debug(f"FAISS ì¶”ê°€ ì™„ë£Œ - ì´ ë²¡í„° ìˆ˜: {self.index.ntotal}")
    
    def similarity_search(self, query_embedding: List[float], k: int = 5) -> List[SearchResult]:
        """
        ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[SearchResult]: ê²€ìƒ‰ ê²°ê³¼ë“¤
        """
        logger.debug(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œì‘ - k: {k}")
        
        if self.db_type == "chroma":
            return self._search_chroma(query_embedding, k)
        elif self.db_type == "faiss":
            return self._search_faiss(query_embedding, k)
    
    def _search_chroma(self, query_embedding: List[float], k: int) -> List[SearchResult]:
        """ChromaDB ê²€ìƒ‰"""
        logger.debug("ChromaDB ê²€ìƒ‰ ìˆ˜í–‰")
        
        try:
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=k,
                include=['documents', 'metadatas', 'distances']
            )
            
            search_results = []
            for i in range(len(results['ids'][0])):
                result = SearchResult(
                    chunk_id=results['ids'][0][i],
                    content=results['documents'][0][i],
                    score=1 - results['distances'][0][i],  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    metadata=results['metadatas'][0][i]
                )
                search_results.append(result)
            
            logger.debug(f"ChromaDB ê²€ìƒ‰ ì™„ë£Œ - ê²°ê³¼ ìˆ˜: {len(search_results)}")
            return search_results
            
        except Exception as e:
            logger.error(f"ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _search_faiss(self, query_embedding: List[float], k: int) -> List[SearchResult]:
        """FAISS ê²€ìƒ‰"""
        logger.debug("FAISS ê²€ìƒ‰ ìˆ˜í–‰")
        
        if self.index is None or self.index.ntotal == 0:
            logger.warning("FAISS ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return []
        
        try:
            query_vector = np.array([query_embedding]).astype('float32')
            
            # ê²€ìƒ‰ ìˆ˜í–‰
            scores, indices = self.index.search(query_vector, k)
            
            search_results = []
            for i in range(len(indices[0])):
                idx = indices[0][i]
                if idx == -1:  # FAISSì—ì„œ ê²°ê³¼ ì—†ìŒì„ ë‚˜íƒ€ëƒ„
                    continue
                
                chunk_data = self.chunk_mapping[str(idx)]
                result = SearchResult(
                    chunk_id=chunk_data['id'],
                    content=chunk_data['content'],
                    score=float(scores[0][i]),
                    metadata=chunk_data['metadata']
                )
                search_results.append(result)
            
            logger.debug(f"FAISS ê²€ìƒ‰ ì™„ë£Œ - ê²°ê³¼ ìˆ˜: {len(search_results)}")
            return search_results
            
        except Exception as e:
            logger.error(f"FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            raise

class RAGGenerator:
    """RAG ì‘ë‹µ ìƒì„±ê¸°"""
    
    def __init__(self, model: str = "gpt-4o-mini", max_context_length: int = 4000):
        """
        RAG ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            model: ì‚¬ìš©í•  LLM ëª¨ë¸
            max_context_length: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
        """
        self.model = model
        self.max_context_length = max_context_length
        self.client = openai.OpenAI(api_key=config.llm.openai_api_key)
        
        logger.info(f"RAG ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸: {model}")
    
    def generate_response(
        self, 
        query: str, 
        search_results: List[SearchResult],
        system_prompt: Optional[str] = None
    ) -> RAGResponse:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆì˜
            search_results: ê²€ìƒ‰ ê²°ê³¼ë“¤
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
            
        Returns:
            RAGResponse: ìƒì„±ëœ ì‘ë‹µ
        """
        start_time = time.time()
        logger.info(f"RAG ì‘ë‹µ ìƒì„± ì‹œì‘ - ì¿¼ë¦¬: {query[:50]}...")
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        if system_prompt is None:
            system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. 
ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
ë‹µë³€í•  ìˆ˜ ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.
ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì°¸ê³ í•œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”."""
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(search_results)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        user_prompt = f"""
ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:

**ì°¸ê³  ë¬¸ì„œ:**
{context}

**ì§ˆë¬¸:** {query}

**ë‹µë³€:**"""
        
        try:
            logger.debug("LLM API í˜¸ì¶œ ì‹œì‘")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=config.llm.temperature,
                max_tokens=config.llm.max_tokens
            )
            
            answer = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            
            processing_time = time.time() - start_time
            
            logger.info(f"RAG ì‘ë‹µ ìƒì„± ì™„ë£Œ - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ, í† í°: {tokens_used}")
            
            # ì‘ë‹µ ë©”íƒ€ë°ì´í„°
            metadata = {
                'model': self.model,
                'search_results_count': len(search_results),
                'context_length': len(context),
                'prompt_length': len(user_prompt),
                'system_prompt_length': len(system_prompt)
            }
            
            return RAGResponse(
                query=query,
                answer=answer,
                sources=search_results,
                processing_time=processing_time,
                tokens_used=tokens_used,
                metadata=metadata
            )
            
        except Exception as e:
            logger.error(f"RAG ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _build_context(self, search_results: List[SearchResult]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        logger.debug(f"ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(search_results)}")
        
        context_parts = []
        current_length = 0
        
        for i, result in enumerate(search_results):
            # ì¶œì²˜ ì •ë³´ í¬í•¨
            source_info = f"[ì¶œì²˜ {i+1}: {result.metadata.get('source_file', 'Unknown')}]"
            content_part = f"{source_info}\n{result.content}\n"
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
            if current_length + len(content_part) > self.max_context_length:
                logger.debug(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œìœ¼ë¡œ {i}ë²ˆì§¸ì—ì„œ ì¤‘ë‹¨")
                break
            
            context_parts.append(content_part)
            current_length += len(content_part)
        
        context = "\n---\n".join(context_parts)
        logger.debug(f"ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ - ê¸¸ì´: {len(context)}ì")
        
        return context

class RAGChatbot:
    """í†µí•© RAG ì±—ë´‡ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        vector_db_type: str = "chroma",
        embedding_model: str = "text-embedding-3-small",
        llm_model: str = "gpt-4o-mini"
    ):
        """
        RAG ì±—ë´‡ ì´ˆê¸°í™”
        
        Args:
            vector_db_type: ë²¡í„° DB ìœ í˜•
            embedding_model: ì„ë² ë”© ëª¨ë¸
            llm_model: LLM ëª¨ë¸
        """
        self.session_id = str(uuid.uuid4())
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.doc_processor = DocumentProcessor(
            chunk_size=config.rag.chunk_size,
            chunk_overlap=config.rag.chunk_overlap
        )
        
        self.embedding_manager = EmbeddingManager(model=embedding_model)
        
        self.vector_db = VectorDatabase(
            db_type=vector_db_type,
            persist_dir=config.rag.chroma_persist_dir
        )
        
        self.rag_generator = RAGGenerator(model=llm_model)
        
        # ì„¸ì…˜ ë©”íƒ€ë°ì´í„°
        self.session_metadata = {
            'session_id': self.session_id,
            'created_at': datetime.now().isoformat(),
            'vector_db_type': vector_db_type,
            'embedding_model': embedding_model,
            'llm_model': llm_model
        }
        
        logger.info(f"RAG ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ - ì„¸ì…˜ ID: {self.session_id}")
    
    def add_document(self, file_path: str) -> Dict[str, Any]:
        """
        ë¬¸ì„œë¥¼ RAG ì‹œìŠ¤í…œì— ì¶”ê°€
        
        Args:
            file_path: ì¶”ê°€í•  ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Dict[str, Any]: ì²˜ë¦¬ ê²°ê³¼ ì •ë³´
        """
        start_time = time.time()
        logger.info(f"ë¬¸ì„œ ì¶”ê°€ ì‹œì‘: {file_path}")
        
        try:
            # 1ë‹¨ê³„: ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹
            logger.info("1ë‹¨ê³„: ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹")
            chunks = self.doc_processor.process_file(file_path)
            
            # 2ë‹¨ê³„: ì„ë² ë”© ìƒì„±
            logger.info("2ë‹¨ê³„: ì„ë² ë”© ìƒì„±")
            chunks_with_embeddings = self.embedding_manager.generate_embeddings(chunks)
            
            # 3ë‹¨ê³„: ë²¡í„° DBì— ì €ì¥
            logger.info("3ë‹¨ê³„: ë²¡í„° DBì— ì €ì¥")
            self.vector_db.add_chunks(chunks_with_embeddings)
            
            processing_time = time.time() - start_time
            
            result = {
                'success': True,
                'file_path': file_path,
                'chunks_count': len(chunks),
                'processing_time': processing_time,
                'session_id': self.session_id
            }
            
            logger.info(f"ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ - ì²­í¬ ìˆ˜: {len(chunks)}, ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            return result
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'file_path': file_path,
                'session_id': self.session_id
            }
    
    def query(self, question: str, k: int = 5) -> RAGResponse:
        """
        ì§ˆì˜ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆì˜
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            RAGResponse: ìƒì„±ëœ ì‘ë‹µ
        """
        start_time = time.time()
        logger.info(f"ì§ˆì˜ ì²˜ë¦¬ ì‹œì‘: {question[:100]}...")
        
        try:
            # 1ë‹¨ê³„: ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            logger.debug("1ë‹¨ê³„: ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±")
            query_embedding = self.embedding_manager.generate_query_embedding(question)
            
            # 2ë‹¨ê³„: ìœ ì‚¬ë„ ê²€ìƒ‰
            logger.debug("2ë‹¨ê³„: ìœ ì‚¬ë„ ê²€ìƒ‰")
            search_results = self.vector_db.similarity_search(query_embedding, k)
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
            logger.info(f"ê²€ìƒ‰ ì™„ë£Œ - ê²°ê³¼ ìˆ˜: {len(search_results)}")
            for i, result in enumerate(search_results):
                logger.debug(f"ê²€ìƒ‰ ê²°ê³¼ {i+1}: ì ìˆ˜ {result.score:.4f}, ì¶œì²˜: {result.metadata.get('source_file', 'Unknown')}")
            
            # 3ë‹¨ê³„: RAG ì‘ë‹µ ìƒì„±
            logger.debug("3ë‹¨ê³„: RAG ì‘ë‹µ ìƒì„±")
            response = self.rag_generator.generate_response(question, search_results)
            
            total_time = time.time() - start_time
            logger.info(f"ì§ˆì˜ ì²˜ë¦¬ ì™„ë£Œ - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
            
            return response
            
        except Exception as e:
            logger.error(f"ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise

# Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤
def streamlit_app():
    """Streamlit ê¸°ë°˜ ì›¹ ì¸í„°í˜ì´ìŠ¤"""
    st.set_page_config(
        page_title="RAG ì±—ë´‡ - ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ",
        page_icon="ğŸ“š",
        layout="wide"
    )
    
    st.title("ğŸ“š RAG ì±—ë´‡ - ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ")
    st.write("PDF, TXT, DOCX, Markdown íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'rag_chatbot' not in st.session_state:
        st.session_state.rag_chatbot = RAGChatbot(
            vector_db_type=config.rag.vector_db_type,
            embedding_model=config.rag.embedding_model
        )
    
    # ì‚¬ì´ë“œë°” - ë¬¸ì„œ ì—…ë¡œë“œ
    with st.sidebar:
        st.header("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")
        
        uploaded_file = st.file_uploader(
            "ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”",
            type=['pdf', 'txt', 'docx', 'md'],
            help="PDF, TXT, DOCX, Markdown íŒŒì¼ì„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        
        if uploaded_file is not None:
            if st.button("ë¬¸ì„œ ì¶”ê°€", type="primary"):
                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                temp_dir = "temp_uploads"
                os.makedirs(temp_dir, exist_ok=True)
                temp_path = os.path.join(temp_dir, uploaded_file.name)
                
                with open(temp_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                # ë¬¸ì„œ ì²˜ë¦¬
                with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    result = st.session_state.rag_chatbot.add_document(temp_path)
                
                if result['success']:
                    st.success(f"ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ!\nì²­í¬ ìˆ˜: {result['chunks_count']}\nì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
                else:
                    st.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {result['error']}")
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.remove(temp_path)
        
        st.header("âš™ï¸ ì„¤ì •")
        
        # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì„¤ì •
        k_results = st.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜", min_value=1, max_value=10, value=5)
        
        # ë²¡í„° DB ì •ë³´
        st.info(f"ë²¡í„° DB: {config.rag.vector_db_type}")
        st.info(f"ì„ë² ë”© ëª¨ë¸: {config.rag.embedding_model}")
    
    # ë©”ì¸ ì˜ì—­ - ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
    st.header("ğŸ’¬ ì§ˆì˜ì‘ë‹µ")
    
    # ì§ˆë¬¸ ì…ë ¥
    question = st.text_input(
        "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
        placeholder="ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”...",
        help="ì—…ë¡œë“œí•œ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
    )
    
    # ì§ˆë¬¸ ì²˜ë¦¬
    if st.button("ì§ˆë¬¸í•˜ê¸°", type="primary") and question:
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                response = st.session_state.rag_chatbot.query(question, k=k_results)
                
                # ë‹µë³€ í‘œì‹œ
                st.subheader("ğŸ¤– ë‹µë³€")
                st.write(response.answer)
                
                # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ì²˜ë¦¬ ì‹œê°„", f"{response.processing_time:.2f}ì´ˆ")
                with col2:
                    st.metric("í† í° ì‚¬ìš©", f"{response.tokens_used}")
                with col3:
                    st.metric("ì°¸ì¡° ë¬¸ì„œ", f"{len(response.sources)}ê°œ")
                
                # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
                with st.expander("ğŸ“‹ ì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš©", expanded=False):
                    for i, source in enumerate(response.sources):
                        st.write(f"**ì°¸ê³  ë¬¸ì„œ {i+1}** (ìœ ì‚¬ë„: {source.score:.4f})")
                        st.write(f"ì¶œì²˜: {source.metadata.get('source_file', 'Unknown')}")
                        st.write(source.content[:500] + "..." if len(source.content) > 500 else source.content)
                        st.write("---")
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # ë””ë²„ê¹… ì •ë³´
    with st.expander("ğŸ”§ ë””ë²„ê·¸ ì •ë³´", expanded=False):
        st.json(st.session_state.rag_chatbot.session_metadata)

# CLI ì¸í„°í˜ì´ìŠ¤
@app.command()
def add_document(
    file_path: str = typer.Argument(..., help="ì¶”ê°€í•  ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ"),
    vector_db: str = typer.Option("chroma", help="ë²¡í„° DB ìœ í˜• (chroma/faiss)"),
    verbose: bool = typer.Option(False, help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
):
    """ë¬¸ì„œë¥¼ RAG ì‹œìŠ¤í…œì— ì¶”ê°€"""
    
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    console.print(f"[bold blue]ë¬¸ì„œ ì¶”ê°€ ì‹œì‘:[/bold blue] {file_path}")
    
    try:
        # RAG ì±—ë´‡ ì´ˆê¸°í™”
        chatbot = RAGChatbot(vector_db_type=vector_db)
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            
            task = progress.add_task("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...", total=None)
            
            # ë¬¸ì„œ ì¶”ê°€
            result = chatbot.add_document(file_path)
            
            progress.remove_task(task)
        
        if result['success']:
            console.print(f"[bold green]âœ“ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ![/bold green]")
            
            # ê²°ê³¼ í…Œì´ë¸”
            table = Table(show_header=True, header_style="bold magenta")
            table.add_column("í•­ëª©", style="cyan")
            table.add_column("ê°’", style="green")
            
            table.add_row("íŒŒì¼ ê²½ë¡œ", result['file_path'])
            table.add_row("ì²­í¬ ìˆ˜", str(result['chunks_count']))
            table.add_row("ì²˜ë¦¬ ì‹œê°„", f"{result['processing_time']:.2f}ì´ˆ")
            table.add_row("ì„¸ì…˜ ID", result['session_id'])
            
            console.print(table)
        else:
            console.print(f"[bold red]âœ— ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨:[/bold red] {result['error']}")
            
    except Exception as e:
        console.print(f"[bold red]ì˜¤ë¥˜ ë°œìƒ:[/bold red] {str(e)}")

@app.command()
def query(
    question: str = typer.Argument(..., help="ì§ˆë¬¸ ë‚´ìš©"),
    k: int = typer.Option(5, help="ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜"),
    vector_db: str = typer.Option("chroma", help="ë²¡í„° DB ìœ í˜• (chroma/faiss)"),
    verbose: bool = typer.Option(False, help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
):
    """RAG ì‹œìŠ¤í…œì— ì§ˆë¬¸í•˜ê¸°"""
    
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    console.print(f"[bold blue]ì§ˆë¬¸:[/bold blue] {question}")
    
    try:
        # RAG ì±—ë´‡ ì´ˆê¸°í™”
        chatbot = RAGChatbot(vector_db_type=vector_db)
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            
            task = progress.add_task("ë‹µë³€ ìƒì„± ì¤‘...", total=None)
            
            # ì§ˆì˜ ì²˜ë¦¬
            response = chatbot.query(question, k=k)
            
            progress.remove_task(task)
        
        # ë‹µë³€ ì¶œë ¥
        console.print(f"\n[bold green]ğŸ¤– ë‹µë³€:[/bold green]")
        console.print(response.answer)
        
        # ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
        console.print(f"\n[bold cyan]ğŸ“Š ì²˜ë¦¬ ì •ë³´:[/bold cyan]")
        table = Table(show_header=True, header_style="bold magenta")
        table.add_column("í•­ëª©", style="cyan")
        table.add_column("ê°’", style="green")
        
        table.add_row("ì²˜ë¦¬ ì‹œê°„", f"{response.processing_time:.2f}ì´ˆ")
        table.add_row("í† í° ì‚¬ìš©", str(response.tokens_used))
        table.add_row("ì°¸ì¡° ë¬¸ì„œ ìˆ˜", str(len(response.sources)))
        
        console.print(table)
        
        # ì°¸ì¡° ë¬¸ì„œ ì¶œë ¥ (verbose ëª¨ë“œ)
        if verbose and response.sources:
            console.print(f"\n[bold yellow]ğŸ“‹ ì°¸ì¡° ë¬¸ì„œ:[/bold yellow]")
            for i, source in enumerate(response.sources):
                console.print(f"\n[bold]ì°¸ì¡° {i+1}[/bold] (ìœ ì‚¬ë„: {source.score:.4f})")
                console.print(f"ì¶œì²˜: {source.metadata.get('source_file', 'Unknown')}")
                console.print(source.content[:200] + "..." if len(source.content) > 200 else source.content)
                
    except Exception as e:
        console.print(f"[bold red]ì˜¤ë¥˜ ë°œìƒ:[/bold red] {str(e)}")

@app.command()
def web():
    """ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰"""
    console.print("[bold green]Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...[/bold green]")
    console.print("ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8501 ì— ì ‘ì†í•˜ì„¸ìš”")
    
    # Streamlit ì•± ì‹¤í–‰
    os.system("streamlit run " + __file__ + " -- web-interface")

@app.command()
def test():
    """RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    console.print("[bold blue]RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘[/bold blue]")
    
    try:
        # í…ŒìŠ¤íŠ¸ìš© RAG ì±—ë´‡ ìƒì„±
        chatbot = RAGChatbot()
        
        # ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸
        console.print("âœ“ RAG ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë”ë¯¸ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
        test_text = """
        ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
        ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ë¡œ, ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.
        ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤.
        """
        
        # ë¬¸ì„œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        chunks = chatbot.doc_processor._smart_chunking(test_text, "test.txt")
        console.print(f"âœ“ ë¬¸ì„œ ì²­í‚¹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì²­í¬ ìˆ˜: {len(chunks)}")
        
        # ì„ë² ë”© í…ŒìŠ¤íŠ¸
        chunks_with_embeddings = chatbot.embedding_manager.generate_embeddings(chunks)
        console.print("âœ“ ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        # ë²¡í„° DB í…ŒìŠ¤íŠ¸
        chatbot.vector_db.add_chunks(chunks_with_embeddings)
        console.print("âœ“ ë²¡í„° DB ì €ì¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        query_embedding = chatbot.embedding_manager.generate_query_embedding("AIë€ ë¬´ì—‡ì¸ê°€ìš”?")
        search_results = chatbot.vector_db.similarity_search(query_embedding, k=2)
        console.print(f"âœ“ ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
        
        # RAG ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸
        response = chatbot.rag_generator.generate_response("AIë€ ë¬´ì—‡ì¸ê°€ìš”?", search_results)
        console.print("âœ“ RAG ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        console.print(f"\n[bold green]ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼![/bold green]")
        console.print(f"ë‹µë³€: {response.answer[:100]}...")
        
    except Exception as e:
        console.print(f"[bold red]í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨:[/bold red] {str(e)}")

if __name__ == "__main__":
    # Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤ ëª¨ë“œ í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "web-interface":
        streamlit_app()
    else:
        # CLI ëª¨ë“œ
        app()
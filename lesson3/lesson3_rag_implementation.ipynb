{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lesson3-title",
   "metadata": {},
   "source": [
    "# 3ì°¨ì‹œ: RAG ì‹¤ë¬´ êµ¬í˜„\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n",
    "- ë²¡í„° ì„ë² ë”© ë° ìœ ì‚¬ë„ ê²€ìƒ‰ êµ¬í˜„\n",
    "- RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "- ì‹¤ì œ PDF ë¬¸ì„œë¥¼ í™œìš©í•œ ì±—ë´‡ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)\n",
    "2. [ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸](#ë¬¸ì„œ-ì²˜ë¦¬-íŒŒì´í”„ë¼ì¸)\n",
    "3. [ë²¡í„° ì„ë² ë”© ì‹œìŠ¤í…œ](#ë²¡í„°-ì„ë² ë”©-ì‹œìŠ¤í…œ)\n",
    "4. [ê²€ìƒ‰ ì‹œìŠ¤í…œ](#ê²€ìƒ‰-ì‹œìŠ¤í…œ)\n",
    "5. [RAG ì‘ë‹µ ìƒì„±](#rag-ì‘ë‹µ-ìƒì„±)\n",
    "6. [í†µí•© í…ŒìŠ¤íŠ¸](#í†µí•©-í…ŒìŠ¤íŠ¸)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì •\n",
    "\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  ê¸°ë³¸ ì„¤ì •ì„ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-requirements",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰)\n",
    "# !pip install pypdf2 python-docx faiss-cpu sentence-transformers chromadb\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "AI ì±—ë´‡ ë©˜í† ë§ - 3ì°¨ì‹œ: RAG ì‹œìŠ¤í…œ êµ¬í˜„\n",
    "Author: AI Chatbot Workshop\n",
    "Date: 2024-08-30\n",
    "Description: ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) ì±—ë´‡ êµ¬í˜„\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import uuid\n",
    "\n",
    "# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# ë¬¸ì„œ ì²˜ë¦¬\n",
    "import PyPDF2\n",
    "import docx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤\n",
    "import faiss\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# ë¡œì»¬ ëª¨ë“ˆ\n",
    "sys.path.append('..')\n",
    "from config import get_config\n",
    "\n",
    "# ì„¤ì • ë¡œë“œ\n",
    "config = get_config()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "document-processing",
   "metadata": {},
   "source": [
    "## 1. ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "### 1.1 ë¬¸ì„œ íŒŒì„œ êµ¬í˜„\n",
    "ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” íŒŒì„œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "document-parser",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"ë¬¸ì„œ ë°ì´í„° êµ¬ì¡°\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    source: str\n",
    "    created_at: datetime\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.supported_formats = ['.pdf', '.txt', '.docx', '.md']\n",
    "    \n",
    "    def process_file(self, file_path: str) -> Document:\n",
    "        \"\"\"\n",
    "        íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ Document ê°ì²´ë¡œ ë³€í™˜\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ\n",
    "            \n",
    "        Returns:\n",
    "            Document: ì²˜ë¦¬ëœ ë¬¸ì„œ ê°ì²´\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        \n",
    "        extension = file_path.suffix.lower()\n",
    "        \n",
    "        if extension not in self.supported_formats:\n",
    "            raise ValueError(f\"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {extension}\")\n",
    "        \n",
    "        self.logger.info(f\"ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file_path}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if extension == '.pdf':\n",
    "                content = self._extract_pdf(file_path)\n",
    "            elif extension == '.docx':\n",
    "                content = self._extract_docx(file_path)\n",
    "            else:  # .txt, .md\n",
    "                content = self._extract_text(file_path)\n",
    "            \n",
    "            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±\n",
    "            metadata = {\n",
    "                'filename': file_path.name,\n",
    "                'file_size': file_path.stat().st_size,\n",
    "                'file_type': extension,\n",
    "                'word_count': len(content.split()),\n",
    "                'char_count': len(content),\n",
    "                'processing_time': time.time() - start_time\n",
    "            }\n",
    "            \n",
    "            document = Document(\n",
    "                id=str(uuid.uuid4()),\n",
    "                content=content,\n",
    "                metadata=metadata,\n",
    "                source=str(file_path),\n",
    "                created_at=datetime.now()\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {metadata['word_count']}ê°œ ë‹¨ì–´\")\n",
    "            return document\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _extract_pdf(self, file_path: Path) -> str:\n",
    "        \"\"\"PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "        text = \"\"\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            for page_num, page in enumerate(reader.pages):\n",
    "                try:\n",
    "                    page_text = page.extract_text()\n",
    "                    text += f\"\\n--- í˜ì´ì§€ {page_num + 1} ---\\n{page_text}\"\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _extract_docx(self, file_path: Path) -> str:\n",
    "        \"\"\"DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        return text.strip()\n",
    "    \n",
    "    def _extract_text(self, file_path: Path) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read().strip()\n",
    "\n",
    "# ë¬¸ì„œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\n",
    "processor = DocumentProcessor()\n",
    "\n",
    "# ìƒ˜í”Œ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\n",
    "sample_text = \"\"\"\n",
    "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ë°œì „\n",
    "\n",
    "ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ê°€ ì¸ê°„ê³¼ ê°™ì€ ì§€ëŠ¥ì ì¸ í–‰ë™ì„ í•˜ë„ë¡ ë§Œë“œëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ, ë°ì´í„°ë¥¼ í†µí•´ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "ìµœê·¼ ë”¥ëŸ¬ë‹ì˜ ë°œì „ìœ¼ë¡œ ìì—°ì–´ ì²˜ë¦¬, ì´ë¯¸ì§€ ì¸ì‹, ìŒì„± ì¸ì‹ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ê´„ëª©í•  ë§Œí•œ ì„±ê³¼ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "íŠ¹íˆ ChatGPTì™€ ê°™ì€ ëŒ€í™”í˜• AIëŠ” ì¼ë°˜ ì‚¬ìš©ìë“¤ë„ ì‰½ê²Œ í™œìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ëª¨ë¸ë¡œ, ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬\n",
    "ë”ìš± ì •í™•í•˜ê³  ìµœì‹ ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "# ìƒ˜í”Œ íŒŒì¼ ìƒì„±\n",
    "sample_file = Path(\"sample_document.txt\")\n",
    "with open(sample_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# ë¬¸ì„œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\n",
    "document = processor.process_file(sample_file)\n",
    "\n",
    "print(f\"ë¬¸ì„œ ID: {document.id}\")\n",
    "print(f\"ë¬¸ì„œ ê¸¸ì´: {len(document.content)} ë¬¸ì\")\n",
    "print(f\"ë©”íƒ€ë°ì´í„°: {document.metadata}\")\n",
    "print(f\"\\në¬¸ì„œ ë‚´ìš© (ì²˜ìŒ 200ì):\")\n",
    "print(document.content[:200] + \"...\")\n",
    "\n",
    "# ì„ì‹œ íŒŒì¼ ì‚­ì œ\n",
    "sample_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunking",
   "metadata": {},
   "source": [
    "### 1.2 ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ êµ¬í˜„\n",
    "ë¬¸ì„œë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ì²­í‚¹ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "document-chunker",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"ë¬¸ì„œ ì²­í¬ ë°ì´í„° êµ¬ì¡°\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    doc_id: str\n",
    "    chunk_index: int\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class SmartChunker:\n",
    "    \"\"\"ìŠ¤ë§ˆíŠ¸ ë¬¸ì„œ ì²­í‚¹ ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, overlap: int = 50):\n",
    "        \"\"\"\n",
    "        ì²­í‚¹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            chunk_size (int): ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)\n",
    "            overlap (int): ì²­í¬ ê°„ ì¤‘ë³µ í¬ê¸°\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def chunk_document(self, document: Document) -> List[DocumentChunk]:\n",
    "        \"\"\"\n",
    "        ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• \n",
    "        \n",
    "        Args:\n",
    "            document (Document): ë¶„í• í•  ë¬¸ì„œ\n",
    "            \n",
    "        Returns:\n",
    "            List[DocumentChunk]: ë¶„í• ëœ ì²­í¬ ëª©ë¡\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"ë¬¸ì„œ ì²­í‚¹ ì‹œì‘: {document.id}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ë¬¸ë‹¨ ê¸°ë°˜ 1ì°¨ ë¶„í• \n",
    "        paragraphs = self._split_by_paragraphs(document.content)\n",
    "        \n",
    "        # í¬ê¸° ê¸°ë°˜ 2ì°¨ ë¶„í• \n",
    "        chunks = self._split_by_size(paragraphs)\n",
    "        \n",
    "        # ì²­í¬ ê°ì²´ ìƒì„±\n",
    "        chunk_objects = []\n",
    "        for i, chunk_content in enumerate(chunks):\n",
    "            chunk = DocumentChunk(\n",
    "                id=f\"{document.id}_chunk_{i}\",\n",
    "                content=chunk_content.strip(),\n",
    "                doc_id=document.id,\n",
    "                chunk_index=i,\n",
    "                metadata={\n",
    "                    'char_count': len(chunk_content),\n",
    "                    'word_count': len(chunk_content.split()),\n",
    "                    'source_file': document.metadata.get('filename', ''),\n",
    "                    'chunk_method': 'smart_chunking'\n",
    "                }\n",
    "            )\n",
    "            chunk_objects.append(chunk)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        self.logger.info(\n",
    "            f\"ì²­í‚¹ ì™„ë£Œ: {len(chunk_objects)}ê°œ ì²­í¬ ìƒì„±, \"\n",
    "            f\"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\"\n",
    "        )\n",
    "        \n",
    "        return chunk_objects\n",
    "    \n",
    "    def _split_by_paragraphs(self, text: str) -> List[str]:\n",
    "        \"\"\"ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í• \"\"\"\n",
    "        # ê°œí–‰ ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ ë¶„í• \n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        # ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ë“¤ì€ í•©ì¹˜ê¸°\n",
    "        merged_paragraphs = []\n",
    "        current_para = \"\"\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            if len(current_para + para) <= self.chunk_size:\n",
    "                current_para += \"\\n\\n\" + para if current_para else para\n",
    "            else:\n",
    "                if current_para:\n",
    "                    merged_paragraphs.append(current_para)\n",
    "                current_para = para\n",
    "        \n",
    "        if current_para:\n",
    "            merged_paragraphs.append(current_para)\n",
    "        \n",
    "        return merged_paragraphs\n",
    "    \n",
    "    def _split_by_size(self, paragraphs: List[str]) -> List[str]:\n",
    "        \"\"\"í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í•  (ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš©)\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            if len(para) <= self.chunk_size:\n",
    "                chunks.append(para)\n",
    "            else:\n",
    "                # í° ë¬¸ë‹¨ì„ ì‘ì€ ì²­í¬ë¡œ ë¶„í• \n",
    "                for i in range(0, len(para), self.chunk_size - self.overlap):\n",
    "                    chunk = para[i:i + self.chunk_size]\n",
    "                    if chunk.strip():\n",
    "                        chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# ì²­í‚¹ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
    "chunker = SmartChunker(chunk_size=200, overlap=50)\n",
    "\n",
    "# ë” ê¸´ ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±\n",
    "long_sample = \"\"\"\n",
    "ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬ì™€ ë°œì „\n",
    "\n",
    "ì¸ê³µì§€ëŠ¥ì˜ ê°œë…ì€ 1950ë…„ëŒ€ ì•¨ëŸ° íŠœë§ì˜ \"íŠœë§ í…ŒìŠ¤íŠ¸\"ë¡œë¶€í„° ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. \n",
    "ì´í›„ 1956ë…„ ë‹¤íŠ¸ë¨¸ìŠ¤ ì»¨í¼ëŸ°ìŠ¤ì—ì„œ \"ì¸ê³µì§€ëŠ¥\"ì´ë¼ëŠ” ìš©ì–´ê°€ ê³µì‹ì ìœ¼ë¡œ ì‚¬ìš©ë˜ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1980ë…„ëŒ€ì—ëŠ” ì „ë¬¸ê°€ ì‹œìŠ¤í…œì´ ì¸ê¸°ë¥¼ ì–»ì—ˆì§€ë§Œ, ì§€ì‹ì„ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•´ì•¼ í•˜ëŠ” í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
    "1990ë…„ëŒ€ë¶€í„° ë¨¸ì‹ ëŸ¬ë‹ ì ‘ê·¼ë²•ì´ ì£¼ëª©ë°›ê¸° ì‹œì‘í–ˆê³ , ë°ì´í„°ë¡œë¶€í„° ìë™ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ ë°œì „í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë”¥ëŸ¬ë‹ì˜ í˜ì‹ \n",
    "\n",
    "2010ë…„ëŒ€ ë“¤ì–´ ë”¥ëŸ¬ë‹ì´ ì»´í“¨í„° ë¹„ì „ê³¼ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.\n",
    "í•©ì„±ê³± ì‹ ê²½ë§(CNN)ì€ ì´ë¯¸ì§€ ì¸ì‹ì—ì„œ, ìˆœí™˜ ì‹ ê²½ë§(RNN)ê³¼ LSTMì€ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "2017ë…„ êµ¬ê¸€ì´ ë°œí‘œí•œ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ë¥¼ ì™„ì „íˆ ë°”ê¾¸ì–´ ë†“ì•˜ìŠµë‹ˆë‹¤.\n",
    "ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ë¬¸ë§¥ì„ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆê³ , ì´ëŠ” BERT, GPT ê°™ì€ ëª¨ë¸ì˜ ê¸°ë°˜ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "í˜„ì¬ì˜ ìƒì„±í˜• AI\n",
    "\n",
    "ChatGPTì˜ ë“±ì¥ìœ¼ë¡œ ìƒì„±í˜• AIê°€ ì¼ë°˜ ëŒ€ì¤‘ì—ê²Œ ë„ë¦¬ ì•Œë ¤ì§€ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ AIì™€ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "RAG(Retrieval-Augmented Generation) ê¸°ìˆ ì€ ì™¸ë¶€ ì§€ì‹ê³¼ ê²°í•©í•˜ì—¬ ë” ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "# ê¸´ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸\n",
    "long_doc = Document(\n",
    "    id=\"test_doc_001\",\n",
    "    content=long_sample,\n",
    "    metadata={'filename': 'ai_history.txt'},\n",
    "    source=\"test\",\n",
    "    created_at=datetime.now()\n",
    ")\n",
    "\n",
    "# ì²­í‚¹ ì‹¤í–‰\n",
    "chunks = chunker.chunk_document(long_doc)\n",
    "\n",
    "print(f\"\\nì´ {len(chunks)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nì²­í¬ {i+1} (ID: {chunk.id}):\")\n",
    "    print(f\"ê¸¸ì´: {chunk.metadata['char_count']}ì, {chunk.metadata['word_count']}ë‹¨ì–´\")\n",
    "    print(f\"ë‚´ìš©: {chunk.content[:100]}...\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector-embedding",
   "metadata": {},
   "source": [
    "## 2. ë²¡í„° ì„ë² ë”© ì‹œìŠ¤í…œ\n",
    "\n",
    "### 2.1 ì„ë² ë”© ìƒì„±ê¸° êµ¬í˜„\n",
    "í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì„ë² ë”© ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): ì‚¬ìš©í•  sentence transformer ëª¨ë¸ëª…\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        try:\n",
    "            self.logger.info(f\"ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {model_name}\")\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "            self.logger.info(f\"ëª¨ë¸ ë¡œë”© ì™„ë£Œ. ì°¨ì›: {self.embedding_dim}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±\n",
    "        \n",
    "        Args:\n",
    "            text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: ìƒì„±ëœ ì„ë² ë”© ë²¡í„°\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            embedding = self.model.encode(text, convert_to_numpy=True)\n",
    "            \n",
    "            self.logger.debug(\n",
    "                f\"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(text)}ì -> {embedding.shape}, \"\n",
    "                f\"ì²˜ë¦¬ ì‹œê°„: {time.time() - start_time:.3f}ì´ˆ\"\n",
    "            )\n",
    "            \n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_batch_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë°°ì¹˜ ì„ë² ë”© ìƒì„±\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ëª©ë¡\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: ìƒì„±ëœ ì„ë² ë”© í–‰ë ¬\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"{len(texts)}ê°œ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹œì‘\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            embeddings = self.model.encode(\n",
    "                texts, \n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            self.logger.info(\n",
    "                f\"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape}, \"\n",
    "                f\"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ \"\n",
    "                f\"(í‰ê·  {processing_time/len(texts):.3f}ì´ˆ/í…ìŠ¤íŠ¸)\"\n",
    "            )\n",
    "            \n",
    "            return embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def compute_similarity(self, embedding1: np.ndarray, \n",
    "                         embedding2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        ë‘ ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "        \n",
    "        Args:\n",
    "            embedding1 (np.ndarray): ì²« ë²ˆì§¸ ì„ë² ë”©\n",
    "            embedding2 (np.ndarray): ë‘ ë²ˆì§¸ ì„ë² ë”©\n",
    "            \n",
    "        Returns:\n",
    "            float: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (-1 ~ 1)\n",
    "        \"\"\"\n",
    "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = dot(A, B) / (norm(A) * norm(B))\n",
    "        dot_product = np.dot(embedding1, embedding2)\n",
    "        norm_a = np.linalg.norm(embedding1)\n",
    "        norm_b = np.linalg.norm(embedding2)\n",
    "        \n",
    "        if norm_a == 0 or norm_b == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# ì„ë² ë”© ìƒì„±ê¸° í…ŒìŠ¤íŠ¸\n",
    "print(\"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... (ì²˜ìŒ ì‹¤í–‰ì‹œ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "embedding_generator = EmbeddingGenerator()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë“¤\n",
    "test_texts = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„°ê°€ ì¸ê°„ê³¼ ê°™ì€ ì§€ëŠ¥ì„ ê°€ì§€ë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.\",\n",
    "    \"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”. ì‚°ì±…í•˜ê¸°ì— ì™„ë²½í•œ ë‚ ì…ë‹ˆë‹¤.\",\n",
    "    \"RAGëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ í˜ì‹ ì ì¸ AI ê¸°ìˆ ì…ë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# ë‹¨ì¼ ì„ë² ë”© í…ŒìŠ¤íŠ¸\n",
    "single_embedding = embedding_generator.generate_embedding(test_texts[0])\n",
    "print(f\"\\në‹¨ì¼ ì„ë² ë”© ê²°ê³¼:\")\n",
    "print(f\"ì°¨ì›: {single_embedding.shape}\")\n",
    "print(f\"ì²« 5ê°œ ê°’: {single_embedding[:5]}\")\n",
    "\n",
    "# ë°°ì¹˜ ì„ë² ë”© í…ŒìŠ¤íŠ¸\n",
    "batch_embeddings = embedding_generator.generate_batch_embeddings(test_texts)\n",
    "print(f\"\\në°°ì¹˜ ì„ë² ë”© ê²°ê³¼:\")\n",
    "print(f\"í˜•íƒœ: {batch_embeddings.shape}\")\n",
    "\n",
    "# ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸\n",
    "print(f\"\\nìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤:\")\n",
    "print(f\"{'':30} \", end=\"\")\n",
    "for i in range(len(test_texts)):\n",
    "    print(f\"í…ìŠ¤íŠ¸{i+1:2}\", end=\"  \")\n",
    "print()\n",
    "\n",
    "for i, text_i in enumerate(test_texts):\n",
    "    print(f\"í…ìŠ¤íŠ¸{i+1} ({text_i[:20]}...) \", end=\"\")\n",
    "    for j in range(len(test_texts)):\n",
    "        similarity = embedding_generator.compute_similarity(\n",
    "            batch_embeddings[i], batch_embeddings[j]\n",
    "        )\n",
    "        print(f\"{similarity:6.3f}\", end=\"  \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector-database",
   "metadata": {},
   "source": [
    "### 2.2 ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬í˜„\n",
    "FAISSì™€ ChromaDBë¥¼ í™œìš©í•œ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vector-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°\"\"\"\n",
    "    chunk: DocumentChunk\n",
    "    score: float\n",
    "    rank: int\n",
    "\n",
    "class VectorDatabase:\n",
    "    \"\"\"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸í„°í˜ì´ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_generator: EmbeddingGenerator):\n",
    "        self.embedding_generator = embedding_generator\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.chunks: List[DocumentChunk] = []\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        \n",
    "    def add_chunks(self, chunks: List[DocumentChunk]):\n",
    "        \"\"\"ì²­í¬ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€\"\"\"\n",
    "        self.logger.info(f\"{len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ ì‹œì‘\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        texts = [chunk.content for chunk in chunks]\n",
    "        \n",
    "        # ì„ë² ë”© ìƒì„±\n",
    "        new_embeddings = self.embedding_generator.generate_batch_embeddings(texts)\n",
    "        \n",
    "        # ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°\n",
    "        if self.embeddings is None:\n",
    "            self.embeddings = new_embeddings\n",
    "            self.chunks = chunks.copy()\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "            self.chunks.extend(chunks)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        self.logger.info(\n",
    "            f\"ì²­í¬ ì¶”ê°€ ì™„ë£Œ: ì´ {len(self.chunks)}ê°œ ì²­í¬, \"\n",
    "            f\"ì„ë² ë”© í˜•íƒœ: {self.embeddings.shape}, \"\n",
    "            f\"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\"\n",
    "        )\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[SearchResult]:\n",
    "        \"\"\"\n",
    "        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ì²­í¬ë“¤ì„ ê²€ìƒ‰\n",
    "        \n",
    "        Args:\n",
    "            query (str): ê²€ìƒ‰ ì¿¼ë¦¬\n",
    "            top_k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜\n",
    "            \n",
    "        Returns:\n",
    "            List[SearchResult]: ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡\n",
    "        \"\"\"\n",
    "        if self.embeddings is None or len(self.chunks) == 0:\n",
    "            self.logger.warning(\"ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤\")\n",
    "            return []\n",
    "        \n",
    "        self.logger.info(f\"ê²€ìƒ‰ ì‹œì‘: '{query[:50]}...', top_k={top_k}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
    "        query_embedding = self.embedding_generator.generate_embedding(query)\n",
    "        \n",
    "        # ëª¨ë“  ì²­í¬ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "        similarities = []\n",
    "        for i, chunk_embedding in enumerate(self.embeddings):\n",
    "            similarity = self.embedding_generator.compute_similarity(\n",
    "                query_embedding, chunk_embedding\n",
    "            )\n",
    "            similarities.append((similarity, i))\n",
    "        \n",
    "        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # ìƒìœ„ kê°œ ê²°ê³¼ ìƒì„±\n",
    "        results = []\n",
    "        for rank, (score, chunk_idx) in enumerate(similarities[:top_k]):\n",
    "            result = SearchResult(\n",
    "                chunk=self.chunks[chunk_idx],\n",
    "                score=score,\n",
    "                rank=rank + 1\n",
    "            )\n",
    "            results.append(result)\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        self.logger.info(\n",
    "            f\"ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼, \"\n",
    "            f\"ìµœê³  ì ìˆ˜: {results[0].score:.4f}, \"\n",
    "            f\"ê²€ìƒ‰ ì‹œê°„: {search_time:.3f}ì´ˆ\"\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´\"\"\"\n",
    "        if not self.chunks:\n",
    "            return {\"total_chunks\": 0, \"total_documents\": 0}\n",
    "        \n",
    "        doc_ids = set(chunk.doc_id for chunk in self.chunks)\n",
    "        \n",
    "        return {\n",
    "            \"total_chunks\": len(self.chunks),\n",
    "            \"total_documents\": len(doc_ids),\n",
    "            \"embedding_dimension\": self.embeddings.shape[1] if self.embeddings is not None else 0,\n",
    "            \"avg_chunk_length\": np.mean([len(chunk.content) for chunk in self.chunks]),\n",
    "            \"total_characters\": sum(len(chunk.content) for chunk in self.chunks)\n",
    "        }\n",
    "\n",
    "# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸\n",
    "vector_db = VectorDatabase(embedding_generator)\n",
    "\n",
    "# ì´ì „ì— ìƒì„±í•œ ì²­í¬ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€\n",
    "vector_db.add_chunks(chunks)\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ í†µê³„\n",
    "stats = vector_db.get_stats()\n",
    "print(f\"\\në²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "test_queries = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬ëŠ” ì–¸ì œë¶€í„° ì‹œì‘ë˜ì—ˆë‚˜ìš”?\",\n",
    "    \"ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”\",\n",
    "    \"RAG ê¸°ìˆ ì´ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = vector_db.search(query, top_k=3)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"\\nìˆœìœ„ {result.rank} (ìœ ì‚¬ë„: {result.score:.4f})\")\n",
    "        print(f\"ì²­í¬ ID: {result.chunk.id}\")\n",
    "        print(f\"ë‚´ìš©: {result.chunk.content[:150]}...\")\n",
    "        print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lesson3-title",
   "metadata": {},
   "source": [
    "# 3차시: RAG 실무 구현\n",
    "\n",
    "## 학습 목표\n",
    "- 문서 처리 파이프라인 구축\n",
    "- 벡터 임베딩 및 유사도 검색 구현\n",
    "- RAG 기반 질의응답 시스템 구축\n",
    "- 실제 PDF 문서를 활용한 챗봇 테스트\n",
    "\n",
    "## 목차\n",
    "1. [환경 설정](#환경-설정)\n",
    "2. [문서 처리 파이프라인](#문서-처리-파이프라인)\n",
    "3. [벡터 임베딩 시스템](#벡터-임베딩-시스템)\n",
    "4. [검색 시스템](#검색-시스템)\n",
    "5. [RAG 응답 생성](#rag-응답-생성)\n",
    "6. [통합 테스트](#통합-테스트)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "\n",
    "필요한 라이브러리를 설치하고 기본 설정을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-requirements",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 (주석 해제하여 실행)\n",
    "# !pip install pypdf2 python-docx faiss-cpu sentence-transformers chromadb\n",
    "\n",
    "print(\"라이브러리 설치 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "AI 챗봇 멘토링 - 3차시: RAG 시스템 구현\n",
    "Author: AI Chatbot Workshop\n",
    "Date: 2024-08-30\n",
    "Description: 문서 기반 검색 증강 생성(RAG) 챗봇 구현\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import uuid\n",
    "\n",
    "# 외부 라이브러리\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# 문서 처리\n",
    "import PyPDF2\n",
    "import docx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 벡터 데이터베이스\n",
    "import faiss\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# 로컬 모듈\n",
    "sys.path.append('..')\n",
    "from config import get_config\n",
    "\n",
    "# 설정 로드\n",
    "config = get_config()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"모든 라이브러리 임포트 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "document-processing",
   "metadata": {},
   "source": [
    "## 1. 문서 처리 파이프라인\n",
    "\n",
    "### 1.1 문서 파서 구현\n",
    "다양한 형식의 문서를 처리할 수 있는 파서를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "document-parser",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"문서 데이터 구조\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    source: str\n",
    "    created_at: datetime\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"문서 처리 파이프라인\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.supported_formats = ['.pdf', '.txt', '.docx', '.md']\n",
    "    \n",
    "    def process_file(self, file_path: str) -> Document:\n",
    "        \"\"\"\n",
    "        파일을 처리하여 Document 객체로 변환\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): 처리할 파일 경로\n",
    "            \n",
    "        Returns:\n",
    "            Document: 처리된 문서 객체\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"파일을 찾을 수 없습니다: {file_path}\")\n",
    "        \n",
    "        extension = file_path.suffix.lower()\n",
    "        \n",
    "        if extension not in self.supported_formats:\n",
    "            raise ValueError(f\"지원되지 않는 파일 형식: {extension}\")\n",
    "        \n",
    "        self.logger.info(f\"문서 처리 시작: {file_path}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if extension == '.pdf':\n",
    "                content = self._extract_pdf(file_path)\n",
    "            elif extension == '.docx':\n",
    "                content = self._extract_docx(file_path)\n",
    "            else:  # .txt, .md\n",
    "                content = self._extract_text(file_path)\n",
    "            \n",
    "            # 문서 메타데이터 생성\n",
    "            metadata = {\n",
    "                'filename': file_path.name,\n",
    "                'file_size': file_path.stat().st_size,\n",
    "                'file_type': extension,\n",
    "                'word_count': len(content.split()),\n",
    "                'char_count': len(content),\n",
    "                'processing_time': time.time() - start_time\n",
    "            }\n",
    "            \n",
    "            document = Document(\n",
    "                id=str(uuid.uuid4()),\n",
    "                content=content,\n",
    "                metadata=metadata,\n",
    "                source=str(file_path),\n",
    "                created_at=datetime.now()\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"문서 처리 완료: {metadata['word_count']}개 단어\")\n",
    "            return document\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"문서 처리 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _extract_pdf(self, file_path: Path) -> str:\n",
    "        \"\"\"PDF 파일에서 텍스트 추출\"\"\"\n",
    "        text = \"\"\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            for page_num, page in enumerate(reader.pages):\n",
    "                try:\n",
    "                    page_text = page.extract_text()\n",
    "                    text += f\"\\n--- 페이지 {page_num + 1} ---\\n{page_text}\"\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"페이지 {page_num + 1} 처리 실패: {e}\")\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _extract_docx(self, file_path: Path) -> str:\n",
    "        \"\"\"DOCX 파일에서 텍스트 추출\"\"\"\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        return text.strip()\n",
    "    \n",
    "    def _extract_text(self, file_path: Path) -> str:\n",
    "        \"\"\"텍스트 파일에서 내용 추출\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read().strip()\n",
    "\n",
    "# 문서 처리 테스트\n",
    "processor = DocumentProcessor()\n",
    "\n",
    "# 샘플 텍스트 파일 생성 (테스트용)\n",
    "sample_text = \"\"\"\n",
    "인공지능과 머신러닝의 발전\n",
    "\n",
    "인공지능(AI)은 컴퓨터가 인간과 같은 지능적인 행동을 하도록 만드는 기술입니다.\n",
    "머신러닝은 인공지능의 한 분야로, 데이터를 통해 패턴을 학습하여 예측이나 결정을 내리는 방법입니다.\n",
    "\n",
    "최근 딥러닝의 발전으로 자연어 처리, 이미지 인식, 음성 인식 등 다양한 분야에서 괄목할 만한 성과를 보이고 있습니다.\n",
    "특히 ChatGPT와 같은 대화형 AI는 일반 사용자들도 쉽게 활용할 수 있게 되었습니다.\n",
    "\n",
    "RAG(Retrieval-Augmented Generation)는 검색 기반 생성 모델로, 외부 지식 베이스에서 관련 정보를 검색하여\n",
    "더욱 정확하고 최신의 정보를 바탕으로 답변을 생성하는 기술입니다.\n",
    "\"\"\"\n",
    "\n",
    "# 샘플 파일 생성\n",
    "sample_file = Path(\"sample_document.txt\")\n",
    "with open(sample_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# 문서 처리 테스트\n",
    "document = processor.process_file(sample_file)\n",
    "\n",
    "print(f\"문서 ID: {document.id}\")\n",
    "print(f\"문서 길이: {len(document.content)} 문자\")\n",
    "print(f\"메타데이터: {document.metadata}\")\n",
    "print(f\"\\n문서 내용 (처음 200자):\")\n",
    "print(document.content[:200] + \"...\")\n",
    "\n",
    "# 임시 파일 삭제\n",
    "sample_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunking",
   "metadata": {},
   "source": [
    "### 1.2 스마트 청킹 구현\n",
    "문서를 의미 있는 단위로 분할하는 청킹 알고리즘을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "document-chunker",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"문서 청크 데이터 구조\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    doc_id: str\n",
    "    chunk_index: int\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class SmartChunker:\n",
    "    \"\"\"스마트 문서 청킹 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, overlap: int = 50):\n",
    "        \"\"\"\n",
    "        청킹 시스템 초기화\n",
    "        \n",
    "        Args:\n",
    "            chunk_size (int): 청크 크기 (문자 수)\n",
    "            overlap (int): 청크 간 중복 크기\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def chunk_document(self, document: Document) -> List[DocumentChunk]:\n",
    "        \"\"\"\n",
    "        문서를 청크로 분할\n",
    "        \n",
    "        Args:\n",
    "            document (Document): 분할할 문서\n",
    "            \n",
    "        Returns:\n",
    "            List[DocumentChunk]: 분할된 청크 목록\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"문서 청킹 시작: {document.id}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 문단 기반 1차 분할\n",
    "        paragraphs = self._split_by_paragraphs(document.content)\n",
    "        \n",
    "        # 크기 기반 2차 분할\n",
    "        chunks = self._split_by_size(paragraphs)\n",
    "        \n",
    "        # 청크 객체 생성\n",
    "        chunk_objects = []\n",
    "        for i, chunk_content in enumerate(chunks):\n",
    "            chunk = DocumentChunk(\n",
    "                id=f\"{document.id}_chunk_{i}\",\n",
    "                content=chunk_content.strip(),\n",
    "                doc_id=document.id,\n",
    "                chunk_index=i,\n",
    "                metadata={\n",
    "                    'char_count': len(chunk_content),\n",
    "                    'word_count': len(chunk_content.split()),\n",
    "                    'source_file': document.metadata.get('filename', ''),\n",
    "                    'chunk_method': 'smart_chunking'\n",
    "                }\n",
    "            )\n",
    "            chunk_objects.append(chunk)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        self.logger.info(\n",
    "            f\"청킹 완료: {len(chunk_objects)}개 청크 생성, \"\n",
    "            f\"처리 시간: {processing_time:.2f}초\"\n",
    "        )\n",
    "        \n",
    "        return chunk_objects\n",
    "    \n",
    "    def _split_by_paragraphs(self, text: str) -> List[str]:\n",
    "        \"\"\"문단 기준으로 텍스트 분할\"\"\"\n",
    "        # 개행 문자 기준으로 분할\n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        # 너무 짧은 문단들은 합치기\n",
    "        merged_paragraphs = []\n",
    "        current_para = \"\"\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            if len(current_para + para) <= self.chunk_size:\n",
    "                current_para += \"\\n\\n\" + para if current_para else para\n",
    "            else:\n",
    "                if current_para:\n",
    "                    merged_paragraphs.append(current_para)\n",
    "                current_para = para\n",
    "        \n",
    "        if current_para:\n",
    "            merged_paragraphs.append(current_para)\n",
    "        \n",
    "        return merged_paragraphs\n",
    "    \n",
    "    def _split_by_size(self, paragraphs: List[str]) -> List[str]:\n",
    "        \"\"\"크기 기준으로 텍스트 분할 (슬라이딩 윈도우 적용)\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            if len(para) <= self.chunk_size:\n",
    "                chunks.append(para)\n",
    "            else:\n",
    "                # 큰 문단을 작은 청크로 분할\n",
    "                for i in range(0, len(para), self.chunk_size - self.overlap):\n",
    "                    chunk = para[i:i + self.chunk_size]\n",
    "                    if chunk.strip():\n",
    "                        chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# 청킹 시스템 테스트\n",
    "chunker = SmartChunker(chunk_size=200, overlap=50)\n",
    "\n",
    "# 더 긴 샘플 문서 생성\n",
    "long_sample = \"\"\"\n",
    "인공지능의 역사와 발전\n",
    "\n",
    "인공지능의 개념은 1950년대 앨런 튜링의 \"튜링 테스트\"로부터 시작되었습니다. \n",
    "이후 1956년 다트머스 컨퍼런스에서 \"인공지능\"이라는 용어가 공식적으로 사용되기 시작했습니다.\n",
    "\n",
    "1980년대에는 전문가 시스템이 인기를 얻었지만, 지식을 수동으로 입력해야 하는 한계가 있었습니다.\n",
    "1990년대부터 머신러닝 접근법이 주목받기 시작했고, 데이터로부터 자동으로 학습하는 방법이 발전했습니다.\n",
    "\n",
    "딥러닝의 혁신\n",
    "\n",
    "2010년대 들어 딥러닝이 컴퓨터 비전과 자연어 처리 분야에서 혁신적인 성과를 보였습니다.\n",
    "합성곱 신경망(CNN)은 이미지 인식에서, 순환 신경망(RNN)과 LSTM은 시계열 데이터 처리에서 뛰어난 성능을 보였습니다.\n",
    "\n",
    "2017년 구글이 발표한 트랜스포머 아키텍처는 자연어 처리 분야를 완전히 바꾸어 놓았습니다.\n",
    "어텐션 메커니즘을 통해 문맥을 더 잘 이해할 수 있게 되었고, 이는 BERT, GPT 같은 모델의 기반이 되었습니다.\n",
    "\n",
    "현재의 생성형 AI\n",
    "\n",
    "ChatGPT의 등장으로 생성형 AI가 일반 대중에게 널리 알려지게 되었습니다.\n",
    "대화형 인터페이스를 통해 누구나 쉽게 AI와 상호작용할 수 있게 되었습니다.\n",
    "RAG(Retrieval-Augmented Generation) 기술은 외부 지식과 결합하여 더 정확한 정보를 제공할 수 있게 해줍니다.\n",
    "\"\"\"\n",
    "\n",
    "# 긴 문서로 테스트\n",
    "long_doc = Document(\n",
    "    id=\"test_doc_001\",\n",
    "    content=long_sample,\n",
    "    metadata={'filename': 'ai_history.txt'},\n",
    "    source=\"test\",\n",
    "    created_at=datetime.now()\n",
    ")\n",
    "\n",
    "# 청킹 실행\n",
    "chunks = chunker.chunk_document(long_doc)\n",
    "\n",
    "print(f\"\\n총 {len(chunks)}개의 청크가 생성되었습니다:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n청크 {i+1} (ID: {chunk.id}):\")\n",
    "    print(f\"길이: {chunk.metadata['char_count']}자, {chunk.metadata['word_count']}단어\")\n",
    "    print(f\"내용: {chunk.content[:100]}...\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector-embedding",
   "metadata": {},
   "source": [
    "## 2. 벡터 임베딩 시스템\n",
    "\n",
    "### 2.1 임베딩 생성기 구현\n",
    "텍스트를 벡터로 변환하는 임베딩 시스템을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"텍스트 임베딩 생성 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        임베딩 생성기 초기화\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): 사용할 sentence transformer 모델명\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        try:\n",
    "            self.logger.info(f\"임베딩 모델 로딩: {model_name}\")\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "            self.logger.info(f\"모델 로딩 완료. 차원: {self.embedding_dim}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"모델 로딩 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        단일 텍스트에 대한 임베딩 생성\n",
    "        \n",
    "        Args:\n",
    "            text (str): 임베딩할 텍스트\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: 생성된 임베딩 벡터\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            embedding = self.model.encode(text, convert_to_numpy=True)\n",
    "            \n",
    "            self.logger.debug(\n",
    "                f\"임베딩 생성 완료: {len(text)}자 -> {embedding.shape}, \"\n",
    "                f\"처리 시간: {time.time() - start_time:.3f}초\"\n",
    "            )\n",
    "            \n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"임베딩 생성 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_batch_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        여러 텍스트에 대한 배치 임베딩 생성\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): 임베딩할 텍스트 목록\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: 생성된 임베딩 행렬\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"{len(texts)}개 텍스트 배치 임베딩 생성 시작\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            embeddings = self.model.encode(\n",
    "                texts, \n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            self.logger.info(\n",
    "                f\"배치 임베딩 생성 완료: {embeddings.shape}, \"\n",
    "                f\"처리 시간: {processing_time:.2f}초 \"\n",
    "                f\"(평균 {processing_time/len(texts):.3f}초/텍스트)\"\n",
    "            )\n",
    "            \n",
    "            return embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"배치 임베딩 생성 실패: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def compute_similarity(self, embedding1: np.ndarray, \n",
    "                         embedding2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        두 임베딩 간 코사인 유사도 계산\n",
    "        \n",
    "        Args:\n",
    "            embedding1 (np.ndarray): 첫 번째 임베딩\n",
    "            embedding2 (np.ndarray): 두 번째 임베딩\n",
    "            \n",
    "        Returns:\n",
    "            float: 코사인 유사도 (-1 ~ 1)\n",
    "        \"\"\"\n",
    "        # 코사인 유사도 = dot(A, B) / (norm(A) * norm(B))\n",
    "        dot_product = np.dot(embedding1, embedding2)\n",
    "        norm_a = np.linalg.norm(embedding1)\n",
    "        norm_b = np.linalg.norm(embedding2)\n",
    "        \n",
    "        if norm_a == 0 or norm_b == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# 임베딩 생성기 테스트\n",
    "print(\"임베딩 모델 로딩 중... (처음 실행시 다운로드로 시간이 걸릴 수 있습니다)\")\n",
    "embedding_generator = EmbeddingGenerator()\n",
    "\n",
    "# 테스트 텍스트들\n",
    "test_texts = [\n",
    "    \"인공지능은 컴퓨터가 인간과 같은 지능을 가지도록 하는 기술입니다.\",\n",
    "    \"머신러닝은 데이터로부터 패턴을 학습하는 인공지능의 한 분야입니다.\",\n",
    "    \"오늘 날씨가 정말 좋네요. 산책하기에 완벽한 날입니다.\",\n",
    "    \"RAG는 검색과 생성을 결합한 혁신적인 AI 기술입니다.\"\n",
    "]\n",
    "\n",
    "# 단일 임베딩 테스트\n",
    "single_embedding = embedding_generator.generate_embedding(test_texts[0])\n",
    "print(f\"\\n단일 임베딩 결과:\")\n",
    "print(f\"차원: {single_embedding.shape}\")\n",
    "print(f\"첫 5개 값: {single_embedding[:5]}\")\n",
    "\n",
    "# 배치 임베딩 테스트\n",
    "batch_embeddings = embedding_generator.generate_batch_embeddings(test_texts)\n",
    "print(f\"\\n배치 임베딩 결과:\")\n",
    "print(f\"형태: {batch_embeddings.shape}\")\n",
    "\n",
    "# 유사도 계산 테스트\n",
    "print(f\"\\n유사도 매트릭스:\")\n",
    "print(f\"{'':30} \", end=\"\")\n",
    "for i in range(len(test_texts)):\n",
    "    print(f\"텍스트{i+1:2}\", end=\"  \")\n",
    "print()\n",
    "\n",
    "for i, text_i in enumerate(test_texts):\n",
    "    print(f\"텍스트{i+1} ({text_i[:20]}...) \", end=\"\")\n",
    "    for j in range(len(test_texts)):\n",
    "        similarity = embedding_generator.compute_similarity(\n",
    "            batch_embeddings[i], batch_embeddings[j]\n",
    "        )\n",
    "        print(f\"{similarity:6.3f}\", end=\"  \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector-database",
   "metadata": {},
   "source": [
    "### 2.2 벡터 데이터베이스 구현\n",
    "FAISS와 ChromaDB를 활용한 벡터 검색 시스템을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vector-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"검색 결과 데이터 구조\"\"\"\n",
    "    chunk: DocumentChunk\n",
    "    score: float\n",
    "    rank: int\n",
    "\n",
    "class VectorDatabase:\n",
    "    \"\"\"벡터 데이터베이스 인터페이스\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_generator: EmbeddingGenerator):\n",
    "        self.embedding_generator = embedding_generator\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.chunks: List[DocumentChunk] = []\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        \n",
    "    def add_chunks(self, chunks: List[DocumentChunk]):\n",
    "        \"\"\"청크들을 데이터베이스에 추가\"\"\"\n",
    "        self.logger.info(f\"{len(chunks)}개 청크 추가 시작\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 텍스트 추출\n",
    "        texts = [chunk.content for chunk in chunks]\n",
    "        \n",
    "        # 임베딩 생성\n",
    "        new_embeddings = self.embedding_generator.generate_batch_embeddings(texts)\n",
    "        \n",
    "        # 기존 데이터와 합치기\n",
    "        if self.embeddings is None:\n",
    "            self.embeddings = new_embeddings\n",
    "            self.chunks = chunks.copy()\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "            self.chunks.extend(chunks)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        self.logger.info(\n",
    "            f\"청크 추가 완료: 총 {len(self.chunks)}개 청크, \"\n",
    "            f\"임베딩 형태: {self.embeddings.shape}, \"\n",
    "            f\"처리 시간: {processing_time:.2f}초\"\n",
    "        )\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[SearchResult]:\n",
    "        \"\"\"\n",
    "        쿼리와 유사한 청크들을 검색\n",
    "        \n",
    "        Args:\n",
    "            query (str): 검색 쿼리\n",
    "            top_k (int): 반환할 결과 수\n",
    "            \n",
    "        Returns:\n",
    "            List[SearchResult]: 검색 결과 목록\n",
    "        \"\"\"\n",
    "        if self.embeddings is None or len(self.chunks) == 0:\n",
    "            self.logger.warning(\"데이터베이스가 비어있습니다\")\n",
    "            return []\n",
    "        \n",
    "        self.logger.info(f\"검색 시작: '{query[:50]}...', top_k={top_k}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 쿼리 임베딩 생성\n",
    "        query_embedding = self.embedding_generator.generate_embedding(query)\n",
    "        \n",
    "        # 모든 청크와의 유사도 계산\n",
    "        similarities = []\n",
    "        for i, chunk_embedding in enumerate(self.embeddings):\n",
    "            similarity = self.embedding_generator.compute_similarity(\n",
    "                query_embedding, chunk_embedding\n",
    "            )\n",
    "            similarities.append((similarity, i))\n",
    "        \n",
    "        # 유사도 순으로 정렬\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 생성\n",
    "        results = []\n",
    "        for rank, (score, chunk_idx) in enumerate(similarities[:top_k]):\n",
    "            result = SearchResult(\n",
    "                chunk=self.chunks[chunk_idx],\n",
    "                score=score,\n",
    "                rank=rank + 1\n",
    "            )\n",
    "            results.append(result)\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        self.logger.info(\n",
    "            f\"검색 완료: {len(results)}개 결과, \"\n",
    "            f\"최고 점수: {results[0].score:.4f}, \"\n",
    "            f\"검색 시간: {search_time:.3f}초\"\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"데이터베이스 통계 정보\"\"\"\n",
    "        if not self.chunks:\n",
    "            return {\"total_chunks\": 0, \"total_documents\": 0}\n",
    "        \n",
    "        doc_ids = set(chunk.doc_id for chunk in self.chunks)\n",
    "        \n",
    "        return {\n",
    "            \"total_chunks\": len(self.chunks),\n",
    "            \"total_documents\": len(doc_ids),\n",
    "            \"embedding_dimension\": self.embeddings.shape[1] if self.embeddings is not None else 0,\n",
    "            \"avg_chunk_length\": np.mean([len(chunk.content) for chunk in self.chunks]),\n",
    "            \"total_characters\": sum(len(chunk.content) for chunk in self.chunks)\n",
    "        }\n",
    "\n",
    "# 벡터 데이터베이스 테스트\n",
    "vector_db = VectorDatabase(embedding_generator)\n",
    "\n",
    "# 이전에 생성한 청크들을 데이터베이스에 추가\n",
    "vector_db.add_chunks(chunks)\n",
    "\n",
    "# 데이터베이스 통계\n",
    "stats = vector_db.get_stats()\n",
    "print(f\"\\n벡터 데이터베이스 통계:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# 검색 테스트\n",
    "test_queries = [\n",
    "    \"인공지능의 역사는 언제부터 시작되었나요?\",\n",
    "    \"딥러닝과 머신러닝의 차이점은 무엇인가요?\",\n",
    "    \"트랜스포머 아키텍처에 대해 알려주세요\",\n",
    "    \"RAG 기술이 무엇인가요?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n🔍 검색 쿼리: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = vector_db.search(query, top_k=3)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"\\n순위 {result.rank} (유사도: {result.score:.4f})\")\n",
    "        print(f\"청크 ID: {result.chunk.id}\")\n",
    "        print(f\"내용: {result.chunk.content[:150]}...\")\n",
    "        print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
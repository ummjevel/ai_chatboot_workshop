#!/usr/bin/env python3
"""
6ì°¨ì‹œ: ì„±ëŠ¥ ìµœì í™” & ëª¨ë‹ˆí„°ë§
Author: AI Chatbot Workshop
Date: 2024-08-30
Description: ì‘ë‹µ ìºì‹±, ë¹„ë™ê¸° ì²˜ë¦¬, í† í° ìµœì í™”, ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬í˜„
"""

import os
import sys
import json
import time
import uuid
import logging
import asyncio
import hashlib
import threading
from typing import Dict, List, Any, Optional, Union, Callable
from dataclasses import dataclass, asdict, field
from datetime import datetime, timedelta
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import wraps, lru_cache
import pickle
import sqlite3

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import streamlit as st
from openai import OpenAI
import redis
import psutil
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import asyncio
import aiohttp

# ë¡œì»¬ ëª¨ë“ˆ
sys.path.append('..')
from config import get_config

# ì„¤ì • ë° ë¡œê¹…
config = get_config()
logger = logging.getLogger(__name__)

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
REQUESTS_TOTAL = Counter('chatbot_requests_total', 'Total requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('chatbot_request_duration_seconds', 'Request duration')
ACTIVE_CONNECTIONS = Gauge('chatbot_active_connections', 'Active connections')
CACHE_HITS = Counter('chatbot_cache_hits_total', 'Cache hits')
CACHE_MISSES = Counter('chatbot_cache_misses_total', 'Cache misses')
TOKEN_USAGE = Counter('chatbot_tokens_used_total', 'Tokens used', ['model'])
ERROR_TOTAL = Counter('chatbot_errors_total', 'Total errors', ['error_type'])

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„°"""
    timestamp: datetime
    request_id: str
    method: str
    endpoint: str
    duration: float
    tokens_used: int
    cache_hit: bool
    error: Optional[str] = None
    user_id: Optional[str] = None
    model: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'timestamp': self.timestamp.isoformat(),
            'request_id': self.request_id,
            'method': self.method,
            'endpoint': self.endpoint,
            'duration': self.duration,
            'tokens_used': self.tokens_used,
            'cache_hit': self.cache_hit,
            'error': self.error,
            'user_id': self.user_id,
            'model': self.model
        }

@dataclass
class SystemMetrics:
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­"""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    disk_usage_percent: float
    active_threads: int
    open_files: int

class CacheManager:
    """ìºì‹± ì‹œìŠ¤í…œ ê´€ë¦¬ì"""
    
    def __init__(self, redis_url: str = None, ttl: int = 3600, max_local_size: int = 1000):
        """
        ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            redis_url: Redis ì—°ê²° URL
            ttl: ìºì‹œ ë§Œë£Œ ì‹œê°„(ì´ˆ)
            max_local_size: ë¡œì»¬ ìºì‹œ ìµœëŒ€ í¬ê¸°
        """
        self.redis_client = None
        self.ttl = ttl
        self.max_local_size = max_local_size
        self.local_cache = {}
        self.local_cache_order = deque()
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'redis_hits': 0,
            'local_hits': 0
        }
        
        self._init_redis(redis_url)
        logger.info(f"ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ - TTL: {ttl}ì´ˆ, ë¡œì»¬ ìµœëŒ€ í¬ê¸°: {max_local_size}")
    
    def _init_redis(self, redis_url: str):
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        try:
            if redis_url:
                self.redis_client = redis.from_url(redis_url, decode_responses=True)
                self.redis_client.ping()
                logger.info("Redis ìºì‹œ ì—°ê²° ì„±ê³µ")
            else:
                logger.info("Redis URLì´ ì œê³µë˜ì§€ ì•ŠìŒ, ë¡œì»¬ ìºì‹œë§Œ ì‚¬ìš©")
        except Exception as e:
            logger.warning(f"Redis ì—°ê²° ì‹¤íŒ¨, ë¡œì»¬ ìºì‹œë§Œ ì‚¬ìš©: {e}")
            self.redis_client = None
    
    def _generate_cache_key(self, prefix: str, data: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ì…ë ¥ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ì—¬ í•´ì‹œ ìƒì„±
        normalized_data = json.dumps(data, sort_keys=True, ensure_ascii=False)
        hash_key = hashlib.sha256(normalized_data.encode()).hexdigest()[:16]
        return f"{prefix}:{hash_key}"
    
    def get(self, prefix: str, data: Dict[str, Any]) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        cache_key = self._generate_cache_key(prefix, data)
        
        # 1. ë¡œì»¬ ìºì‹œ í™•ì¸
        if cache_key in self.local_cache:
            cache_entry = self.local_cache[cache_key]
            if datetime.now() < cache_entry['expires_at']:
                self.cache_stats['hits'] += 1
                self.cache_stats['local_hits'] += 1
                CACHE_HITS.inc()
                logger.debug(f"ë¡œì»¬ ìºì‹œ íˆíŠ¸: {cache_key}")
                return cache_entry['data']
            else:
                # ë§Œë£Œëœ ë¡œì»¬ ìºì‹œ ì‚­ì œ
                del self.local_cache[cache_key]
                self.local_cache_order.remove(cache_key)
        
        # 2. Redis ìºì‹œ í™•ì¸
        if self.redis_client:
            try:
                cached_data = self.redis_client.get(cache_key)
                if cached_data:
                    data_obj = json.loads(cached_data)
                    # ë¡œì»¬ ìºì‹œì—ë„ ì €ì¥
                    self._store_local(cache_key, data_obj)
                    self.cache_stats['hits'] += 1
                    self.cache_stats['redis_hits'] += 1
                    CACHE_HITS.inc()
                    logger.debug(f"Redis ìºì‹œ íˆíŠ¸: {cache_key}")
                    return data_obj
            except Exception as e:
                logger.error(f"Redis ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # ìºì‹œ ë¯¸ìŠ¤
        self.cache_stats['misses'] += 1
        CACHE_MISSES.inc()
        logger.debug(f"ìºì‹œ ë¯¸ìŠ¤: {cache_key}")
        return None
    
    def set(self, prefix: str, data: Dict[str, Any], value: Any):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        cache_key = self._generate_cache_key(prefix, data)
        
        # ë¡œì»¬ ìºì‹œì— ì €ì¥
        self._store_local(cache_key, value)
        
        # Redisì— ì €ì¥
        if self.redis_client:
            try:
                serialized_value = json.dumps(value, ensure_ascii=False, default=str)
                self.redis_client.setex(cache_key, self.ttl, serialized_value)
                logger.debug(f"Redisì— ì €ì¥: {cache_key}")
            except Exception as e:
                logger.error(f"Redis ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _store_local(self, cache_key: str, value: Any):
        """ë¡œì»¬ ìºì‹œì— ì €ì¥"""
        # í¬ê¸° ì œí•œ í™•ì¸
        if len(self.local_cache) >= self.max_local_size:
            # LRU ì •ì±…ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = self.local_cache_order.popleft()
            if oldest_key in self.local_cache:
                del self.local_cache[oldest_key]
        
        expires_at = datetime.now() + timedelta(seconds=self.ttl)
        self.local_cache[cache_key] = {
            'data': value,
            'expires_at': expires_at
        }
        
        if cache_key in self.local_cache_order:
            self.local_cache_order.remove(cache_key)
        self.local_cache_order.append(cache_key)
        
        logger.debug(f"ë¡œì»¬ ìºì‹œì— ì €ì¥: {cache_key}")
    
    def clear(self):
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        self.local_cache.clear()
        self.local_cache_order.clear()
        
        if self.redis_client:
            try:
                # íŠ¹ì • íŒ¨í„´ì˜ í‚¤ë§Œ ì‚­ì œ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•˜ê²Œ)
                keys = self.redis_client.keys("*")
                if keys:
                    self.redis_client.delete(*keys)
                logger.info("Redis ìºì‹œ ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"Redis ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        logger.info("ìºì‹œ ì „ì²´ ì‚­ì œ ì™„ë£Œ")
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = (self.cache_stats['hits'] / total_requests) if total_requests > 0 else 0
        
        return {
            'total_requests': total_requests,
            'cache_hits': self.cache_stats['hits'],
            'cache_misses': self.cache_stats['misses'],
            'hit_rate': hit_rate,
            'redis_hits': self.cache_stats['redis_hits'],
            'local_hits': self.cache_stats['local_hits'],
            'local_cache_size': len(self.local_cache),
            'redis_connected': self.redis_client is not None
        }

class TokenOptimizer:
    """í† í° ì‚¬ìš©ëŸ‰ ìµœì í™”"""
    
    def __init__(self):
        self.optimization_stats = {
            'original_tokens': 0,
            'optimized_tokens': 0,
            'savings_percentage': 0.0
        }
        logger.info("í† í° ìµœì í™”ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def optimize_prompt(self, prompt: str, max_tokens: int = 4000) -> str:
        """í”„ë¡¬í”„íŠ¸ í† í° ìµœì í™”"""
        original_length = len(prompt)
        
        if original_length <= max_tokens:
            return prompt
        
        # ìµœì í™” ì „ëµë“¤ ì ìš©
        optimized = prompt
        
        # 1. ì¤‘ë³µ ê³µë°± ì œê±°
        import re
        optimized = re.sub(r'\s+', ' ', optimized)
        
        # 2. ë¶ˆí•„ìš”í•œ êµ¬ë¬¸ ì œê±°
        unnecessary_phrases = [
            'please note that',
            'it is important to',
            'you should understand that',
            'as mentioned before',
            'in other words',
            'to summarize'
        ]
        
        for phrase in unnecessary_phrases:
            optimized = optimized.replace(phrase, '')
        
        # 3. ë¬¸ì¥ ì••ì¶•
        if len(optimized) > max_tokens:
            sentences = optimized.split('. ')
            # ì¤‘ìš”ë„ê°€ ë†’ì€ ë¬¸ì¥ ìš°ì„  ìœ ì§€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            important_sentences = []
            current_length = 0
            
            for sentence in sentences:
                if current_length + len(sentence) <= max_tokens:
                    important_sentences.append(sentence)
                    current_length += len(sentence)
                else:
                    break
            
            optimized = '. '.join(important_sentences)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.optimization_stats['original_tokens'] += original_length
        self.optimization_stats['optimized_tokens'] += len(optimized)
        
        if self.optimization_stats['original_tokens'] > 0:
            savings = 1 - (self.optimization_stats['optimized_tokens'] / self.optimization_stats['original_tokens'])
            self.optimization_stats['savings_percentage'] = savings * 100
        
        if len(optimized) < original_length:
            logger.info(f"í”„ë¡¬í”„íŠ¸ ìµœì í™”: {original_length} -> {len(optimized)} ë¬¸ì ({((original_length - len(optimized)) / original_length * 100):.1f}% ì ˆì•½)")
        
        return optimized
    
    def optimize_messages(self, messages: List[Dict[str, str]], max_total_tokens: int = 3000) -> List[Dict[str, str]]:
        """ë©”ì‹œì§€ ëª©ë¡ ìµœì í™”"""
        if not messages:
            return messages
        
        # ì „ì²´ í† í° ìˆ˜ ì¶”ì •
        total_tokens = sum(len(msg.get('content', '')) for msg in messages)
        
        if total_tokens <= max_total_tokens:
            return messages
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ë³´ì¡´
        system_messages = [msg for msg in messages if msg.get('role') == 'system']
        other_messages = [msg for msg in messages if msg.get('role') != 'system']
        
        # ìµœê·¼ ë©”ì‹œì§€ ìš°ì„  ë³´ì¡´
        optimized_messages = system_messages[:]
        current_tokens = sum(len(msg.get('content', '')) for msg in system_messages)
        
        for msg in reversed(other_messages):
            msg_tokens = len(msg.get('content', ''))
            if current_tokens + msg_tokens <= max_total_tokens:
                optimized_messages.insert(-len(system_messages) if system_messages else 0, msg)
                current_tokens += msg_tokens
            else:
                break
        
        logger.info(f"ë©”ì‹œì§€ ìµœì í™”: {len(messages)} -> {len(optimized_messages)} ê°œ")
        return optimized_messages
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """ìµœì í™” í†µê³„ ë°˜í™˜"""
        return self.optimization_stats.copy()

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, db_path: str = "performance_metrics.db"):
        self.db_path = db_path
        self.metrics_buffer = deque(maxlen=1000)  # ìµœê·¼ 1000ê°œ ë©”íŠ¸ë¦­ ìœ ì§€
        self.system_metrics = []
        self.monitoring_active = False
        self.monitor_thread = None
        
        self._init_database()
        logger.info(f"ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™” ì™„ë£Œ - DB: {db_path}")
    
    def _init_database(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ DB ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT,
                    request_id TEXT,
                    method TEXT,
                    endpoint TEXT,
                    duration REAL,
                    tokens_used INTEGER,
                    cache_hit BOOLEAN,
                    error TEXT,
                    user_id TEXT,
                    model TEXT
                )
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS system_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT,
                    cpu_percent REAL,
                    memory_percent REAL,
                    memory_used_mb REAL,
                    disk_usage_percent REAL,
                    active_threads INTEGER,
                    open_files INTEGER
                )
            ''')
            
            # ì¸ë±ìŠ¤ ìƒì„±
            conn.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON performance_metrics(timestamp)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_endpoint ON performance_metrics(endpoint)')
    
    def record_metric(self, metric: PerformanceMetrics):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.metrics_buffer.append(metric)
        
        # Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        REQUESTS_TOTAL.labels(method=metric.method, endpoint=metric.endpoint).inc()
        REQUEST_DURATION.observe(metric.duration)
        TOKEN_USAGE.labels(model=metric.model or 'unknown').inc(metric.tokens_used)
        
        if metric.cache_hit:
            CACHE_HITS.inc()
        else:
            CACHE_MISSES.inc()
        
        if metric.error:
            ERROR_TOTAL.labels(error_type=type(metric.error).__name__).inc()
        
        # ì£¼ê¸°ì ìœ¼ë¡œ DBì— ì €ì¥
        if len(self.metrics_buffer) >= 100:
            self._flush_metrics_to_db()
    
    def _flush_metrics_to_db(self):
        """ë©”íŠ¸ë¦­ ë²„í¼ë¥¼ DBì— ì €ì¥"""
        if not self.metrics_buffer:
            return
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                metrics_data = [
                    (
                        metric.timestamp.isoformat(),
                        metric.request_id,
                        metric.method,
                        metric.endpoint,
                        metric.duration,
                        metric.tokens_used,
                        metric.cache_hit,
                        metric.error,
                        metric.user_id,
                        metric.model
                    )
                    for metric in self.metrics_buffer
                ]
                
                conn.executemany('''
                    INSERT INTO performance_metrics 
                    (timestamp, request_id, method, endpoint, duration, tokens_used, 
                     cache_hit, error, user_id, model)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', metrics_data)
                
                self.metrics_buffer.clear()
                logger.debug(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ {len(metrics_data)}ê°œë¥¼ DBì— ì €ì¥")
                
        except Exception as e:
            logger.error(f"ë©”íŠ¸ë¦­ DB ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def start_system_monitoring(self, interval: int = 60):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        self.monitor_thread = threading.Thread(
            target=self._system_monitor_loop, 
            args=(interval,),
            daemon=True
        )
        self.monitor_thread.start()
        logger.info(f"ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘ - ê°„ê²©: {interval}ì´ˆ")
    
    def stop_system_monitoring(self):
        """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        logger.info("ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _system_monitor_loop(self, interval: int):
        """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring_active:
            try:
                # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                cpu_percent = psutil.cpu_percent(interval=1)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')
                
                # í”„ë¡œì„¸ìŠ¤ ì •ë³´
                process = psutil.Process()
                num_threads = process.num_threads()
                num_fds = len(process.open_files())
                
                system_metric = SystemMetrics(
                    timestamp=datetime.now(),
                    cpu_percent=cpu_percent,
                    memory_percent=memory.percent,
                    memory_used_mb=memory.used / (1024 * 1024),
                    disk_usage_percent=disk.percent,
                    active_threads=num_threads,
                    open_files=num_fds
                )
                
                self.system_metrics.append(system_metric)
                
                # ì˜¤ë˜ëœ ë©”íŠ¸ë¦­ ì œê±° (ìµœê·¼ 24ì‹œê°„ë§Œ ìœ ì§€)
                cutoff_time = datetime.now() - timedelta(hours=24)
                self.system_metrics = [
                    m for m in self.system_metrics 
                    if m.timestamp > cutoff_time
                ]
                
                # DBì— ì €ì¥
                self._save_system_metric(system_metric)
                
                # Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                ACTIVE_CONNECTIONS.set(num_threads)
                
                time.sleep(interval)
                
            except Exception as e:
                logger.error(f"ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(interval)
    
    def _save_system_metric(self, metric: SystemMetrics):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ DB ì €ì¥"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    INSERT INTO system_metrics 
                    (timestamp, cpu_percent, memory_percent, memory_used_mb, 
                     disk_usage_percent, active_threads, open_files)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    metric.timestamp.isoformat(),
                    metric.cpu_percent,
                    metric.memory_percent,
                    metric.memory_used_mb,
                    metric.disk_usage_percent,
                    metric.active_threads,
                    metric.open_files
                ))
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ DB ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_performance_stats(self, hours: int = 1) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                
                # ê¸°ë³¸ í†µê³„
                cursor = conn.execute('''
                    SELECT 
                        COUNT(*) as total_requests,
                        AVG(duration) as avg_duration,
                        MAX(duration) as max_duration,
                        MIN(duration) as min_duration,
                        SUM(tokens_used) as total_tokens,
                        AVG(tokens_used) as avg_tokens,
                        SUM(CASE WHEN cache_hit = 1 THEN 1 ELSE 0 END) as cache_hits,
                        SUM(CASE WHEN error IS NOT NULL THEN 1 ELSE 0 END) as error_count
                    FROM performance_metrics 
                    WHERE timestamp > ?
                ''', (cutoff_time.isoformat(),))
                
                stats = dict(cursor.fetchone())
                
                # ì—”ë“œí¬ì¸íŠ¸ë³„ í†µê³„
                cursor = conn.execute('''
                    SELECT endpoint, COUNT(*) as count, AVG(duration) as avg_duration
                    FROM performance_metrics 
                    WHERE timestamp > ?
                    GROUP BY endpoint
                    ORDER BY count DESC
                ''', (cutoff_time.isoformat(),))
                
                endpoint_stats = [dict(row) for row in cursor.fetchall()]
                
                # ì—ëŸ¬ í†µê³„
                cursor = conn.execute('''
                    SELECT error, COUNT(*) as count
                    FROM performance_metrics 
                    WHERE timestamp > ? AND error IS NOT NULL
                    GROUP BY error
                    ORDER BY count DESC
                ''', (cutoff_time.isoformat(),))
                
                error_stats = [dict(row) for row in cursor.fetchall()]
                
                return {
                    'period_hours': hours,
                    'basic_stats': stats,
                    'endpoint_stats': endpoint_stats,
                    'error_stats': error_stats,
                    'cache_hit_rate': (stats['cache_hits'] / stats['total_requests']) if stats['total_requests'] > 0 else 0
                }
                
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_system_stats(self, hours: int = 1) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                
                cursor = conn.execute('''
                    SELECT 
                        AVG(cpu_percent) as avg_cpu,
                        MAX(cpu_percent) as max_cpu,
                        AVG(memory_percent) as avg_memory,
                        MAX(memory_percent) as max_memory,
                        AVG(memory_used_mb) as avg_memory_mb,
                        MAX(active_threads) as max_threads,
                        MAX(open_files) as max_open_files
                    FROM system_metrics 
                    WHERE timestamp > ?
                ''', (cutoff_time.isoformat(),))
                
                return dict(cursor.fetchone())
                
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

def performance_decorator(endpoint: str, cache_prefix: str = None):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            request_id = str(uuid.uuid4())
            start_time = time.time()
            cache_hit = False
            error = None
            tokens_used = 0
            
            # ìºì‹± í™•ì¸
            if cache_prefix and hasattr(wrapper, '_cache_manager'):
                cached_result = wrapper._cache_manager.get(cache_prefix, kwargs)
                if cached_result is not None:
                    cache_hit = True
                    duration = time.time() - start_time
                    
                    if hasattr(wrapper, '_monitor'):
                        metric = PerformanceMetrics(
                            timestamp=datetime.now(),
                            request_id=request_id,
                            method='cached',
                            endpoint=endpoint,
                            duration=duration,
                            tokens_used=0,
                            cache_hit=True
                        )
                        wrapper._monitor.record_metric(metric)
                    
                    return cached_result
            
            try:
                # í•¨ìˆ˜ ì‹¤í–‰
                result = func(*args, **kwargs)
                
                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ (ê²°ê³¼ì—ì„œ)
                if isinstance(result, dict) and 'tokens_used' in result:
                    tokens_used = result['tokens_used']
                
                # ê²°ê³¼ ìºì‹±
                if cache_prefix and hasattr(wrapper, '_cache_manager'):
                    wrapper._cache_manager.set(cache_prefix, kwargs, result)
                
                return result
                
            except Exception as e:
                error = str(e)
                raise
            
            finally:
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
                duration = time.time() - start_time
                
                if hasattr(wrapper, '_monitor'):
                    metric = PerformanceMetrics(
                        timestamp=datetime.now(),
                        request_id=request_id,
                        method=func.__name__,
                        endpoint=endpoint,
                        duration=duration,
                        tokens_used=tokens_used,
                        cache_hit=cache_hit,
                        error=error
                    )
                    wrapper._monitor.record_metric(metric)
        
        return wrapper
    return decorator

class AsyncChatbot:
    """ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ëŠ” ìµœì í™”ëœ ì±—ë´‡"""
    
    def __init__(self):
        self.client = OpenAI(api_key=config.llm.openai_api_key)
        self.cache_manager = CacheManager(redis_url=config.get_redis_url())
        self.token_optimizer = TokenOptimizer()
        self.performance_monitor = PerformanceMonitor()
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.performance_monitor.start_system_monitoring()
        
        logger.info("ë¹„ë™ê¸° ìµœì í™” ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ")
    
    @performance_decorator(endpoint="/chat", cache_prefix="chat_response")
    def chat(self, user_message: str, user_id: str = None, use_cache: bool = True) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì±„íŒ… ì‘ë‹µ"""
        start_time = time.time()
        
        # ìºì‹œ í‚¤ ìƒì„±ì„ ìœ„í•œ ë°ì´í„°
        cache_data = {
            'message': user_message,
            'model': config.llm.openai_model,
            'temperature': config.llm.temperature
        }
        
        # ìºì‹œ í™•ì¸
        if use_cache:
            cached_response = self.cache_manager.get("chat_response", cache_data)
            if cached_response:
                logger.info(f"ìºì‹œëœ ì‘ë‹µ ë°˜í™˜: {user_message[:50]}...")
                return cached_response
        
        try:
            # í”„ë¡¬í”„íŠ¸ ìµœì í™”
            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": user_message}
            ]
            
            optimized_messages = self.token_optimizer.optimize_messages(messages)
            
            # OpenAI API í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model=config.llm.openai_model,
                messages=optimized_messages,
                temperature=config.llm.temperature,
                max_tokens=config.llm.max_tokens
            )
            
            processing_time = time.time() - start_time
            tokens_used = response.usage.total_tokens
            ai_response = response.choices[0].message.content
            
            result = {
                'success': True,
                'response': ai_response,
                'tokens_used': tokens_used,
                'processing_time': processing_time,
                'cached': False,
                'user_id': user_id
            }
            
            # ê²°ê³¼ ìºì‹±
            if use_cache:
                self.cache_manager.set("chat_response", cache_data, result)
            
            logger.info(f"ìƒˆ ì‘ë‹µ ìƒì„±: {user_message[:50]}... ({tokens_used} í† í°, {processing_time:.2f}ì´ˆ)")
            
            return result
            
        except Exception as e:
            error_msg = f"ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
            logger.error(error_msg)
            
            return {
                'success': False,
                'error': error_msg,
                'processing_time': time.time() - start_time,
                'tokens_used': 0,
                'user_id': user_id
            }
    
    async def chat_async(self, user_message: str, user_id: str = None) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì±„íŒ… ì²˜ë¦¬"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, self.chat, user_message, user_id)
    
    async def batch_chat(self, messages: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ë©”ì‹œì§€ ë³‘ë ¬ ì²˜ë¦¬"""
        tasks = [
            self.chat_async(msg['content'], msg.get('user_id'))
            for msg in messages
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    'success': False,
                    'error': str(result),
                    'message_index': i
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ í†µê³„"""
        cache_stats = self.cache_manager.get_stats()
        optimization_stats = self.token_optimizer.get_optimization_stats()
        performance_stats = self.performance_monitor.get_performance_stats()
        system_stats = self.performance_monitor.get_system_stats()
        
        return {
            'cache': cache_stats,
            'optimization': optimization_stats,
            'performance': performance_stats,
            'system': system_stats,
            'timestamp': datetime.now().isoformat()
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.performance_monitor.stop_system_monitoring()
        self.performance_monitor._flush_metrics_to_db()
        self.executor.shutdown(wait=True)
        logger.info("ì±—ë´‡ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

class DashboardGenerator:
    """ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ ìƒì„±ê¸°"""
    
    def __init__(self, chatbot: AsyncChatbot):
        self.chatbot = chatbot
    
    def generate_performance_charts(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„±"""
        try:
            # ì„±ëŠ¥ ë°ì´í„° ì¡°íšŒ
            with sqlite3.connect(self.chatbot.performance_monitor.db_path) as conn:
                # ìµœê·¼ 24ì‹œê°„ ì‘ë‹µ ì‹œê°„ íŠ¸ë Œë“œ
                df_response = pd.read_sql_query('''
                    SELECT 
                        datetime(timestamp) as time,
                        AVG(duration) as avg_duration,
                        COUNT(*) as request_count
                    FROM performance_metrics 
                    WHERE timestamp > datetime('now', '-24 hours')
                    GROUP BY datetime(timestamp, 'localtime', 'start of hour')
                    ORDER BY time
                ''', conn)
                
                # ì—”ë“œí¬ì¸íŠ¸ë³„ ì„±ëŠ¥
                df_endpoint = pd.read_sql_query('''
                    SELECT 
                        endpoint,
                        COUNT(*) as request_count,
                        AVG(duration) as avg_duration,
                        SUM(tokens_used) as total_tokens
                    FROM performance_metrics 
                    WHERE timestamp > datetime('now', '-24 hours')
                    GROUP BY endpoint
                    ORDER BY request_count DESC
                ''', conn)
                
                # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ íŠ¸ë Œë“œ
                df_system = pd.read_sql_query('''
                    SELECT 
                        datetime(timestamp) as time,
                        AVG(cpu_percent) as cpu_percent,
                        AVG(memory_percent) as memory_percent
                    FROM system_metrics 
                    WHERE timestamp > datetime('now', '-24 hours')
                    GROUP BY datetime(timestamp, 'localtime', 'start of hour')
                    ORDER BY time
                ''', conn)
            
            # ì°¨íŠ¸ ë°ì´í„° ë°˜í™˜
            return {
                'response_time_trend': df_response.to_dict('records'),
                'endpoint_performance': df_endpoint.to_dict('records'),
                'system_resource_trend': df_system.to_dict('records')
            }
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def create_matplotlib_charts(self, save_path: str = "performance_dashboard.png"):
        """Matplotlibì„ ì‚¬ìš©í•œ ì°¨íŠ¸ ìƒì„±"""
        try:
            chart_data = self.generate_performance_charts()
            
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('ì±—ë´‡ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ', fontsize=16)
            
            # 1. ì‘ë‹µ ì‹œê°„ íŠ¸ë Œë“œ
            if chart_data.get('response_time_trend'):
                df = pd.DataFrame(chart_data['response_time_trend'])
                if not df.empty:
                    ax1.plot(df['time'], df['avg_duration'], marker='o')
                    ax1.set_title('í‰ê·  ì‘ë‹µ ì‹œê°„ íŠ¸ë Œë“œ')
                    ax1.set_xlabel('ì‹œê°„')
                    ax1.set_ylabel('ì‘ë‹µ ì‹œê°„ (ì´ˆ)')
                    ax1.tick_params(axis='x', rotation=45)
            
            # 2. ì—”ë“œí¬ì¸íŠ¸ë³„ ìš”ì²­ ìˆ˜
            if chart_data.get('endpoint_performance'):
                df = pd.DataFrame(chart_data['endpoint_performance'])
                if not df.empty:
                    ax2.bar(df['endpoint'], df['request_count'])
                    ax2.set_title('ì—”ë“œí¬ì¸íŠ¸ë³„ ìš”ì²­ ìˆ˜')
                    ax2.set_xlabel('ì—”ë“œí¬ì¸íŠ¸')
                    ax2.set_ylabel('ìš”ì²­ ìˆ˜')
            
            # 3. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 
            if chart_data.get('system_resource_trend'):
                df = pd.DataFrame(chart_data['system_resource_trend'])
                if not df.empty:
                    ax3.plot(df['time'], df['cpu_percent'], label='CPU %', marker='o')
                    ax3.plot(df['time'], df['memory_percent'], label='Memory %', marker='s')
                    ax3.set_title('ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ ')
                    ax3.set_xlabel('ì‹œê°„')
                    ax3.set_ylabel('ì‚¬ìš©ë¥  (%)')
                    ax3.legend()
                    ax3.tick_params(axis='x', rotation=45)
            
            # 4. ìºì‹œ í†µê³„
            cache_stats = self.chatbot.cache_manager.get_stats()
            if cache_stats['total_requests'] > 0:
                labels = ['Cache Hits', 'Cache Misses']
                sizes = [cache_stats['cache_hits'], cache_stats['cache_misses']]
                colors = ['lightgreen', 'lightcoral']
                ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')
                ax4.set_title('ìºì‹œ íˆíŠ¸ìœ¨')
            
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ì„±ëŠ¥ ì°¨íŠ¸ ì €ì¥: {save_path}")
            return save_path
            
        except Exception as e:
            logger.error(f"ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

# Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤
def streamlit_app():
    """Streamlit ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"""
    st.set_page_config(
        page_title="ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    st.title("ğŸ“Š AI ì±—ë´‡ ë©˜í† ë§ - 6ì°¨ì‹œ: ì„±ëŠ¥ ìµœì í™” & ëª¨ë‹ˆí„°ë§")
    st.write("ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ê³¼ ìµœì í™” ê¸°ëŠ¥ì„ ì²´í—˜í•´ë³´ì„¸ìš”!")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = AsyncChatbot()
        st.session_state.dashboard = DashboardGenerator(st.session_state.chatbot)
    
    # ì‚¬ì´ë“œë°” - ì„¤ì • ë° í†µê³„
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # ìºì‹œ ê´€ë¦¬
        st.subheader("ğŸ—„ï¸ ìºì‹œ ê´€ë¦¬")
        cache_stats = st.session_state.chatbot.cache_manager.get_stats()
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ìºì‹œ íˆíŠ¸ìœ¨", f"{cache_stats['hit_rate']:.1%}")
            st.metric("ìºì‹œ íˆíŠ¸", cache_stats['cache_hits'])
        with col2:
            st.metric("ì´ ìš”ì²­", cache_stats['total_requests'])
            st.metric("ìºì‹œ ë¯¸ìŠ¤", cache_stats['cache_misses'])
        
        if st.button("ìºì‹œ ì „ì²´ ì‚­ì œ"):
            st.session_state.chatbot.cache_manager.clear()
            st.success("ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()
        
        st.divider()
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        st.subheader("ğŸ–¥ï¸ ì‹œìŠ¤í…œ ìƒíƒœ")
        system_stats = st.session_state.chatbot.performance_monitor.get_system_stats()
        
        if system_stats:
            st.metric("í‰ê·  CPU", f"{system_stats.get('avg_cpu', 0):.1f}%")
            st.metric("í‰ê·  ë©”ëª¨ë¦¬", f"{system_stats.get('avg_memory', 0):.1f}%")
            st.metric("ìµœëŒ€ ìŠ¤ë ˆë“œ", f"{system_stats.get('max_threads', 0)}")
        
        st.divider()
        
        # í† í° ìµœì í™” í†µê³„
        st.subheader("ğŸ¯ í† í° ìµœì í™”")
        opt_stats = st.session_state.chatbot.token_optimizer.get_optimization_stats()
        
        if opt_stats['original_tokens'] > 0:
            st.metric("ì ˆì•½ë¥ ", f"{opt_stats['savings_percentage']:.1f}%")
            st.metric("ì›ë³¸ í† í°", f"{opt_stats['original_tokens']:,}")
            st.metric("ìµœì í™” í† í°", f"{opt_stats['optimized_tokens']:,}")
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„¤ì •
        st.divider()
        st.subheader("ğŸ“¡ ëª¨ë‹ˆí„°ë§")
        
        if st.button("Prometheus ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘"):
            try:
                start_http_server(8000)
                st.success("Prometheus ì„œë²„ê°€ í¬íŠ¸ 8000ì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.info("http://localhost:8000/metrics ì—ì„œ í™•ì¸ ê°€ëŠ¥")
            except Exception as e:
                st.error(f"ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
    
    # ë©”ì¸ ì˜ì—­
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¬ ì±„íŒ…", "ğŸ“Š ì„±ëŠ¥ ì°¨íŠ¸", "ğŸ“ˆ ì‹¤ì‹œê°„ í†µê³„", "ğŸ§ª ë°°ì¹˜ í…ŒìŠ¤íŠ¸"])
    
    with tab1:
        st.header("ğŸ’¬ ìµœì í™”ëœ ì±„íŒ…")
        
        # ì±„íŒ… ì˜µì…˜
        col1, col2 = st.columns([3, 1])
        with col1:
            use_cache = st.checkbox("ìºì‹± ì‚¬ìš©", value=True)
        with col2:
            user_id = st.text_input("ì‚¬ìš©ì ID", value="test_user")
        
        # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
        if user_input := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
            # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
            with st.chat_message("user"):
                st.write(user_input)
            
            # AI ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘..."):
                    result = st.session_state.chatbot.chat(
                        user_message=user_input,
                        user_id=user_id,
                        use_cache=use_cache
                    )
                
                if result['success']:
                    st.write(result['response'])
                    
                    # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ì²˜ë¦¬ ì‹œê°„", f"{result['processing_time']:.3f}ì´ˆ")
                    with col2:
                        st.metric("í† í° ì‚¬ìš©", result['tokens_used'])
                    with col3:
                        cache_status = "ìºì‹œë¨" if result.get('cached') else "ì‹ ê·œ"
                        st.metric("ìºì‹œ ìƒíƒœ", cache_status)
                    with col4:
                        st.metric("ì‚¬ìš©ì ID", result.get('user_id', 'N/A'))
                else:
                    st.error(f"ì˜¤ë¥˜: {result.get('error')}")
    
    with tab2:
        st.header("ğŸ“Š ì„±ëŠ¥ ë¶„ì„ ì°¨íŠ¸")
        
        if st.button("ì°¨íŠ¸ ìƒˆë¡œê³ ì¹¨"):
            with st.spinner("ì°¨íŠ¸ ìƒì„± ì¤‘..."):
                chart_path = st.session_state.dashboard.create_matplotlib_charts()
                if chart_path and os.path.exists(chart_path):
                    st.image(chart_path)
                    st.success("ì°¨íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    st.warning("ì°¨íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        
        # ì„±ëŠ¥ í†µê³„ í…Œì´ë¸”
        st.subheader("ğŸ“‹ ì„±ëŠ¥ í†µê³„ ìš”ì•½")
        perf_stats = st.session_state.chatbot.performance_monitor.get_performance_stats(hours=24)
        
        if perf_stats and perf_stats.get('basic_stats'):
            basic_stats = perf_stats['basic_stats']
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì´ ìš”ì²­", basic_stats.get('total_requests', 0))
            with col2:
                st.metric("í‰ê·  ì‘ë‹µì‹œê°„", f"{basic_stats.get('avg_duration', 0):.3f}ì´ˆ")
            with col3:
                st.metric("ì´ í† í°", f"{basic_stats.get('total_tokens', 0):,}")
            with col4:
                st.metric("ì—ëŸ¬ ìˆ˜", basic_stats.get('error_count', 0))
            
            # ì—”ë“œí¬ì¸íŠ¸ë³„ í†µê³„
            if perf_stats.get('endpoint_stats'):
                st.subheader("ğŸ”— ì—”ë“œí¬ì¸íŠ¸ë³„ ì„±ëŠ¥")
                df = pd.DataFrame(perf_stats['endpoint_stats'])
                st.dataframe(df, use_container_width=True)
    
    with tab3:
        st.header("ğŸ“ˆ ì‹¤ì‹œê°„ í†µê³„")
        
        # ìë™ ìƒˆë¡œê³ ì¹¨
        auto_refresh = st.checkbox("ìë™ ìƒˆë¡œê³ ì¹¨ (30ì´ˆ)", value=False)
        if auto_refresh:
            time.sleep(30)
            st.rerun()
        
        if st.button("ì§€ê¸ˆ ìƒˆë¡œê³ ì¹¨"):
            st.rerun()
        
        # ì¢…í•© ì„±ëŠ¥ í†µê³„
        all_stats = st.session_state.chatbot.get_performance_stats()
        
        # ìºì‹œ ì„±ëŠ¥
        st.subheader("ğŸ—„ï¸ ìºì‹œ ì„±ëŠ¥")
        cache_stats = all_stats.get('cache', {})
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ìºì‹œ íˆíŠ¸ìœ¨", f"{cache_stats.get('hit_rate', 0):.1%}")
        with col2:
            st.metric("Redis ì—°ê²°", "âœ…" if cache_stats.get('redis_connected') else "âŒ")
        with col3:
            st.metric("ë¡œì»¬ ìºì‹œ í¬ê¸°", cache_stats.get('local_cache_size', 0))
        
        # í† í° ìµœì í™”
        st.subheader("ğŸ¯ í† í° ìµœì í™”")
        opt_stats = all_stats.get('optimization', {})
        
        if opt_stats.get('original_tokens', 0) > 0:
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ì ˆì•½ëœ í† í°", f"{opt_stats['original_tokens'] - opt_stats['optimized_tokens']:,}")
            with col2:
                st.metric("ì ˆì•½ë¥ ", f"{opt_stats['savings_percentage']:.1f}%")
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
        st.subheader("ğŸ–¥ï¸ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤")
        system_stats = all_stats.get('system', {})
        
        if system_stats:
            col1, col2, col3 = st.columns(3)
            with col1:
                cpu_percent = system_stats.get('avg_cpu', 0)
                st.metric("í‰ê·  CPU", f"{cpu_percent:.1f}%", 
                         delta=f"{cpu_percent - 50:.1f}%" if cpu_percent > 50 else None)
            with col2:
                memory_percent = system_stats.get('avg_memory', 0)
                st.metric("í‰ê·  ë©”ëª¨ë¦¬", f"{memory_percent:.1f}%",
                         delta=f"{memory_percent - 60:.1f}%" if memory_percent > 60 else None)
            with col3:
                st.metric("ìµœëŒ€ ìŠ¤ë ˆë“œ", system_stats.get('max_threads', 0))
    
    with tab4:
        st.header("ğŸ§ª ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        
        st.write("ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        
        # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì…ë ¥
        default_messages = """ì•ˆë…•í•˜ì„¸ìš”! 
Pythonì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.
AIëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?
ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?
í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì¶”ì²œí•´ì£¼ì„¸ìš”."""
        
        test_messages = st.text_area(
            "í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ë“¤ (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„):",
            value=default_messages,
            height=150
        )
        
        batch_size = st.slider("ë°°ì¹˜ í¬ê¸°", min_value=1, max_value=10, value=5)
        
        if st.button("ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"):
            messages = [msg.strip() for msg in test_messages.split('\n') if msg.strip()]
            messages = messages[:batch_size]  # ë°°ì¹˜ í¬ê¸°ë¡œ ì œí•œ
            
            if messages:
                with st.spinner(f"{len(messages)}ê°œ ë©”ì‹œì§€ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘..."):
                    start_time = time.time()
                    
                    # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ì‹œì§€ í¬ë§·
                    batch_messages = [
                        {'content': msg, 'user_id': f'batch_user_{i}'}
                        for i, msg in enumerate(messages)
                    ]
                    
                    # ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    try:
                        results = loop.run_until_complete(
                            st.session_state.chatbot.batch_chat(batch_messages)
                        )
                        
                        total_time = time.time() - start_time
                        successful_results = [r for r in results if r.get('success')]
                        
                        # ê²°ê³¼ ìš”ì•½
                        st.success(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("ì´ ì²˜ë¦¬ ì‹œê°„", f"{total_time:.2f}ì´ˆ")
                        with col2:
                            st.metric("ì„±ê³µë¥ ", f"{len(successful_results)}/{len(results)}")
                        with col3:
                            avg_time = sum(r.get('processing_time', 0) for r in successful_results) / len(successful_results) if successful_results else 0
                            st.metric("í‰ê·  ì‘ë‹µì‹œê°„", f"{avg_time:.3f}ì´ˆ")
                        with col4:
                            total_tokens = sum(r.get('tokens_used', 0) for r in successful_results)
                            st.metric("ì´ í† í°", f"{total_tokens:,}")
                        
                        # ìƒì„¸ ê²°ê³¼
                        st.subheader("ğŸ“‹ ìƒì„¸ ê²°ê³¼")
                        for i, (msg, result) in enumerate(zip(messages, results)):
                            with st.expander(f"ë©”ì‹œì§€ {i+1}: {msg[:50]}..."):
                                if result.get('success'):
                                    st.write(f"**ì‘ë‹µ**: {result['response'][:200]}...")
                                    st.write(f"**ì²˜ë¦¬ì‹œê°„**: {result['processing_time']:.3f}ì´ˆ")
                                    st.write(f"**í† í°**: {result['tokens_used']}")
                                else:
                                    st.error(f"ì˜¤ë¥˜: {result.get('error')}")
                    
                    finally:
                        loop.close()
            else:
                st.warning("í…ŒìŠ¤íŠ¸í•  ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

def run_performance_test():
    """CLI ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("=== ì„±ëŠ¥ ìµœì í™” ì±—ë´‡ í…ŒìŠ¤íŠ¸ ===")
    
    chatbot = AsyncChatbot()
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
    test_messages = [
        "ì•ˆë…•í•˜ì„¸ìš”!",
        "Pythonì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.",
        "AIëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?",
        "í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
        "ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ê³µë¶€ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”."
    ]
    
    print(f"\nğŸ§ª {len(test_messages)}ê°œ ë©”ì‹œì§€ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ìºì‹œ ì—†ì´ ì²« ë²ˆì§¸ ì‹¤í–‰
    print("\n1ï¸âƒ£ ìºì‹œ ì—†ì´ ì‹¤í–‰:")
    chatbot.cache_manager.clear()
    
    start_time = time.time()
    results_no_cache = []
    
    for i, message in enumerate(test_messages, 1):
        print(f"  {i}. ì²˜ë¦¬ ì¤‘: {message[:30]}...")
        result = chatbot.chat(message, use_cache=False)
        results_no_cache.append(result)
        print(f"     ì‘ë‹µì‹œê°„: {result['processing_time']:.3f}ì´ˆ, í† í°: {result['tokens_used']}")
    
    total_time_no_cache = time.time() - start_time
    
    # ìºì‹œ ì‚¬ìš© ë‘ ë²ˆì§¸ ì‹¤í–‰
    print("\n2ï¸âƒ£ ìºì‹œ ì‚¬ìš©í•˜ì—¬ ë™ì¼ ë©”ì‹œì§€ ì¬ì‹¤í–‰:")
    
    start_time = time.time()
    results_with_cache = []
    
    for i, message in enumerate(test_messages, 1):
        print(f"  {i}. ì²˜ë¦¬ ì¤‘: {message[:30]}...")
        result = chatbot.chat(message, use_cache=True)
        results_with_cache.append(result)
        cache_status = "ìºì‹œë¨" if result.get('cached') else "ì‹ ê·œ"
        print(f"     ì‘ë‹µì‹œê°„: {result['processing_time']:.3f}ì´ˆ, ìƒíƒœ: {cache_status}")
    
    total_time_with_cache = time.time() - start_time
    
    # ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸:")
    
    batch_messages = [{'content': msg, 'user_id': f'test_user_{i}'} for i, msg in enumerate(test_messages)]
    
    start_time = time.time()
    
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        batch_results = loop.run_until_complete(chatbot.batch_chat(batch_messages))
        batch_time = time.time() - start_time
        
        successful_batch = [r for r in batch_results if r.get('success')]
        
        print(f"  ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(successful_batch)}/{len(batch_results)} ì„±ê³µ")
        print(f"  ì´ ì²˜ë¦¬ì‹œê°„: {batch_time:.3f}ì´ˆ")
        print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {sum(r.get('processing_time', 0) for r in successful_batch) / len(successful_batch):.3f}ì´ˆ")
    
    finally:
        loop.close()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    
    avg_time_no_cache = sum(r['processing_time'] for r in results_no_cache) / len(results_no_cache)
    avg_time_with_cache = sum(r['processing_time'] for r in results_with_cache) / len(results_with_cache)
    total_tokens_no_cache = sum(r['tokens_used'] for r in results_no_cache)
    
    print(f"ğŸš« ìºì‹œ ì—†ìŒ:")
    print(f"  - ì´ ì‹œê°„: {total_time_no_cache:.3f}ì´ˆ")
    print(f"  - í‰ê·  ì‘ë‹µì‹œê°„: {avg_time_no_cache:.3f}ì´ˆ")
    print(f"  - ì´ í† í°: {total_tokens_no_cache:,}")
    
    print(f"\nâœ… ìºì‹œ ì‚¬ìš©:")
    print(f"  - ì´ ì‹œê°„: {total_time_with_cache:.3f}ì´ˆ")
    print(f"  - í‰ê·  ì‘ë‹µì‹œê°„: {avg_time_with_cache:.3f}ì´ˆ")
    print(f"  - ì‹œê°„ ì ˆì•½: {((total_time_no_cache - total_time_with_cache) / total_time_no_cache * 100):.1f}%")
    
    print(f"\nâš¡ ë¹„ë™ê¸° ë°°ì¹˜:")
    print(f"  - ì´ ì‹œê°„: {batch_time:.3f}ì´ˆ")
    print(f"  - ìˆœì°¨ ëŒ€ë¹„ í–¥ìƒ: {((total_time_no_cache - batch_time) / total_time_no_cache * 100):.1f}%")
    
    # ìºì‹œ ë° ìµœì í™” í†µê³„
    cache_stats = chatbot.cache_manager.get_stats()
    opt_stats = chatbot.token_optimizer.get_optimization_stats()
    
    print(f"\nğŸ“ˆ ìºì‹œ ì„±ëŠ¥:")
    print(f"  - íˆíŠ¸ìœ¨: {cache_stats['hit_rate']:.1%}")
    print(f"  - ì´ ìš”ì²­: {cache_stats['total_requests']}")
    print(f"  - ìºì‹œ íˆíŠ¸: {cache_stats['cache_hits']}")
    
    if opt_stats['original_tokens'] > 0:
        print(f"\nğŸ¯ í† í° ìµœì í™”:")
        print(f"  - ì ˆì•½ë¥ : {opt_stats['savings_percentage']:.1f}%")
        print(f"  - ì›ë³¸ í† í°: {opt_stats['original_tokens']:,}")
        print(f"  - ìµœì í™” í† í°: {opt_stats['optimized_tokens']:,}")
    
    # ì •ë¦¬
    chatbot.cleanup()
    print("\nâœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        run_performance_test()
    elif len(sys.argv) > 1 and sys.argv[1] == "prometheus":
        # Prometheus ëª¨ë‹ˆí„°ë§ ì„œë²„ë§Œ ì‹œì‘
        print("Prometheus ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘ ì¤‘...")
        start_http_server(8000)
        print("ë©”íŠ¸ë¦­ ì„œë²„ê°€ í¬íŠ¸ 8000ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
        print("http://localhost:8000/metrics ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
        print("ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    else:
        # Streamlit ëª¨ë“œ
        streamlit_app()